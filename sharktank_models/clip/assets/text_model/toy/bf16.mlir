module @module {
  util.global private @__auto.text_model.embeddings.token_embedding.weight = #stream.parameter.named<"model"::"text_model.embeddings.token_embedding.weight"> : tensor<11x65xbf16>
  util.global private @__auto.text_model.embeddings.position_embedding.weight = #stream.parameter.named<"model"::"text_model.embeddings.position_embedding.weight"> : tensor<17x65xbf16>
  util.global private @__auto.text_model.encoder.layers.0.layer_norm1.weight = #stream.parameter.named<"model"::"text_model.encoder.layers.0.layer_norm1.weight"> : tensor<65xbf16>
  util.global private @__auto.text_model.encoder.layers.0.layer_norm1.bias = #stream.parameter.named<"model"::"text_model.encoder.layers.0.layer_norm1.bias"> : tensor<65xbf16>
  util.global private @__auto.text_model.encoder.layers.0.self_attn.q_proj.weight = #stream.parameter.named<"model"::"text_model.encoder.layers.0.self_attn.q_proj.weight"> : tensor<65x65xbf16>
  util.global private @__auto.text_model.encoder.layers.0.self_attn.q_proj.bias = #stream.parameter.named<"model"::"text_model.encoder.layers.0.self_attn.q_proj.bias"> : tensor<65xbf16>
  util.global private @__auto.text_model.encoder.layers.0.self_attn.k_proj.weight = #stream.parameter.named<"model"::"text_model.encoder.layers.0.self_attn.k_proj.weight"> : tensor<65x65xbf16>
  util.global private @__auto.text_model.encoder.layers.0.self_attn.k_proj.bias = #stream.parameter.named<"model"::"text_model.encoder.layers.0.self_attn.k_proj.bias"> : tensor<65xbf16>
  util.global private @__auto.text_model.encoder.layers.0.self_attn.v_proj.weight = #stream.parameter.named<"model"::"text_model.encoder.layers.0.self_attn.v_proj.weight"> : tensor<65x65xbf16>
  util.global private @__auto.text_model.encoder.layers.0.self_attn.v_proj.bias = #stream.parameter.named<"model"::"text_model.encoder.layers.0.self_attn.v_proj.bias"> : tensor<65xbf16>
  util.global private @__auto.text_model.encoder.layers.0.self_attn.out_proj.weight = #stream.parameter.named<"model"::"text_model.encoder.layers.0.self_attn.out_proj.weight"> : tensor<65x65xbf16>
  util.global private @__auto.text_model.encoder.layers.0.self_attn.out_proj.bias = #stream.parameter.named<"model"::"text_model.encoder.layers.0.self_attn.out_proj.bias"> : tensor<65xbf16>
  util.global private @__auto.text_model.encoder.layers.0.layer_norm2.weight = #stream.parameter.named<"model"::"text_model.encoder.layers.0.layer_norm2.weight"> : tensor<65xbf16>
  util.global private @__auto.text_model.encoder.layers.0.layer_norm2.bias = #stream.parameter.named<"model"::"text_model.encoder.layers.0.layer_norm2.bias"> : tensor<65xbf16>
  util.global private @__auto.text_model.encoder.layers.0.mlp.fc1.weight = #stream.parameter.named<"model"::"text_model.encoder.layers.0.mlp.fc1.weight"> : tensor<7x65xbf16>
  util.global private @__auto.text_model.encoder.layers.0.mlp.fc1.bias = #stream.parameter.named<"model"::"text_model.encoder.layers.0.mlp.fc1.bias"> : tensor<7xbf16>
  util.global private @__auto.text_model.encoder.layers.0.mlp.fc2.weight = #stream.parameter.named<"model"::"text_model.encoder.layers.0.mlp.fc2.weight"> : tensor<65x7xbf16>
  util.global private @__auto.text_model.encoder.layers.0.mlp.fc2.bias = #stream.parameter.named<"model"::"text_model.encoder.layers.0.mlp.fc2.bias"> : tensor<65xbf16>
  util.global private @__auto.text_model.encoder.layers.1.layer_norm1.weight = #stream.parameter.named<"model"::"text_model.encoder.layers.1.layer_norm1.weight"> : tensor<65xbf16>
  util.global private @__auto.text_model.encoder.layers.1.layer_norm1.bias = #stream.parameter.named<"model"::"text_model.encoder.layers.1.layer_norm1.bias"> : tensor<65xbf16>
  util.global private @__auto.text_model.encoder.layers.1.self_attn.q_proj.weight = #stream.parameter.named<"model"::"text_model.encoder.layers.1.self_attn.q_proj.weight"> : tensor<65x65xbf16>
  util.global private @__auto.text_model.encoder.layers.1.self_attn.q_proj.bias = #stream.parameter.named<"model"::"text_model.encoder.layers.1.self_attn.q_proj.bias"> : tensor<65xbf16>
  util.global private @__auto.text_model.encoder.layers.1.self_attn.k_proj.weight = #stream.parameter.named<"model"::"text_model.encoder.layers.1.self_attn.k_proj.weight"> : tensor<65x65xbf16>
  util.global private @__auto.text_model.encoder.layers.1.self_attn.k_proj.bias = #stream.parameter.named<"model"::"text_model.encoder.layers.1.self_attn.k_proj.bias"> : tensor<65xbf16>
  util.global private @__auto.text_model.encoder.layers.1.self_attn.v_proj.weight = #stream.parameter.named<"model"::"text_model.encoder.layers.1.self_attn.v_proj.weight"> : tensor<65x65xbf16>
  util.global private @__auto.text_model.encoder.layers.1.self_attn.v_proj.bias = #stream.parameter.named<"model"::"text_model.encoder.layers.1.self_attn.v_proj.bias"> : tensor<65xbf16>
  util.global private @__auto.text_model.encoder.layers.1.self_attn.out_proj.weight = #stream.parameter.named<"model"::"text_model.encoder.layers.1.self_attn.out_proj.weight"> : tensor<65x65xbf16>
  util.global private @__auto.text_model.encoder.layers.1.self_attn.out_proj.bias = #stream.parameter.named<"model"::"text_model.encoder.layers.1.self_attn.out_proj.bias"> : tensor<65xbf16>
  util.global private @__auto.text_model.encoder.layers.1.layer_norm2.weight = #stream.parameter.named<"model"::"text_model.encoder.layers.1.layer_norm2.weight"> : tensor<65xbf16>
  util.global private @__auto.text_model.encoder.layers.1.layer_norm2.bias = #stream.parameter.named<"model"::"text_model.encoder.layers.1.layer_norm2.bias"> : tensor<65xbf16>
  util.global private @__auto.text_model.encoder.layers.1.mlp.fc1.weight = #stream.parameter.named<"model"::"text_model.encoder.layers.1.mlp.fc1.weight"> : tensor<7x65xbf16>
  util.global private @__auto.text_model.encoder.layers.1.mlp.fc1.bias = #stream.parameter.named<"model"::"text_model.encoder.layers.1.mlp.fc1.bias"> : tensor<7xbf16>
  util.global private @__auto.text_model.encoder.layers.1.mlp.fc2.weight = #stream.parameter.named<"model"::"text_model.encoder.layers.1.mlp.fc2.weight"> : tensor<65x7xbf16>
  util.global private @__auto.text_model.encoder.layers.1.mlp.fc2.bias = #stream.parameter.named<"model"::"text_model.encoder.layers.1.mlp.fc2.bias"> : tensor<65xbf16>
  util.global private @__auto.text_model.final_layer_norm.weight = #stream.parameter.named<"model"::"text_model.final_layer_norm.weight"> : tensor<65xbf16>
  util.global private @__auto.text_model.final_layer_norm.bias = #stream.parameter.named<"model"::"text_model.final_layer_norm.bias"> : tensor<65xbf16>
  func.func @forward_bs4(%arg0: !torch.vtensor<[4,17],si64>) -> (!torch.vtensor<[4,17,65],bf16>, !torch.vtensor<[4,65],bf16>) attributes {torch.assume_strict_symbolic_shapes} {
    %__auto.text_model.embeddings.token_embedding.weight = util.global.load @__auto.text_model.embeddings.token_embedding.weight : tensor<11x65xbf16>
    %0 = torch_c.from_builtin_tensor %__auto.text_model.embeddings.token_embedding.weight : tensor<11x65xbf16> -> !torch.vtensor<[11,65],bf16>
    %__auto.text_model.embeddings.position_embedding.weight = util.global.load @__auto.text_model.embeddings.position_embedding.weight : tensor<17x65xbf16>
    %1 = torch_c.from_builtin_tensor %__auto.text_model.embeddings.position_embedding.weight : tensor<17x65xbf16> -> !torch.vtensor<[17,65],bf16>
    %__auto.text_model.encoder.layers.0.layer_norm1.weight = util.global.load @__auto.text_model.encoder.layers.0.layer_norm1.weight : tensor<65xbf16>
    %2 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.0.layer_norm1.weight : tensor<65xbf16> -> !torch.vtensor<[65],bf16>
    %__auto.text_model.encoder.layers.0.layer_norm1.bias = util.global.load @__auto.text_model.encoder.layers.0.layer_norm1.bias : tensor<65xbf16>
    %3 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.0.layer_norm1.bias : tensor<65xbf16> -> !torch.vtensor<[65],bf16>
    %__auto.text_model.encoder.layers.0.self_attn.q_proj.weight = util.global.load @__auto.text_model.encoder.layers.0.self_attn.q_proj.weight : tensor<65x65xbf16>
    %4 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.0.self_attn.q_proj.weight : tensor<65x65xbf16> -> !torch.vtensor<[65,65],bf16>
    %__auto.text_model.encoder.layers.0.self_attn.q_proj.bias = util.global.load @__auto.text_model.encoder.layers.0.self_attn.q_proj.bias : tensor<65xbf16>
    %5 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.0.self_attn.q_proj.bias : tensor<65xbf16> -> !torch.vtensor<[65],bf16>
    %__auto.text_model.encoder.layers.0.self_attn.k_proj.weight = util.global.load @__auto.text_model.encoder.layers.0.self_attn.k_proj.weight : tensor<65x65xbf16>
    %6 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.0.self_attn.k_proj.weight : tensor<65x65xbf16> -> !torch.vtensor<[65,65],bf16>
    %__auto.text_model.encoder.layers.0.self_attn.k_proj.bias = util.global.load @__auto.text_model.encoder.layers.0.self_attn.k_proj.bias : tensor<65xbf16>
    %7 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.0.self_attn.k_proj.bias : tensor<65xbf16> -> !torch.vtensor<[65],bf16>
    %__auto.text_model.encoder.layers.0.self_attn.v_proj.weight = util.global.load @__auto.text_model.encoder.layers.0.self_attn.v_proj.weight : tensor<65x65xbf16>
    %8 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.0.self_attn.v_proj.weight : tensor<65x65xbf16> -> !torch.vtensor<[65,65],bf16>
    %__auto.text_model.encoder.layers.0.self_attn.v_proj.bias = util.global.load @__auto.text_model.encoder.layers.0.self_attn.v_proj.bias : tensor<65xbf16>
    %9 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.0.self_attn.v_proj.bias : tensor<65xbf16> -> !torch.vtensor<[65],bf16>
    %__auto.text_model.encoder.layers.0.self_attn.out_proj.weight = util.global.load @__auto.text_model.encoder.layers.0.self_attn.out_proj.weight : tensor<65x65xbf16>
    %10 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.0.self_attn.out_proj.weight : tensor<65x65xbf16> -> !torch.vtensor<[65,65],bf16>
    %__auto.text_model.encoder.layers.0.self_attn.out_proj.bias = util.global.load @__auto.text_model.encoder.layers.0.self_attn.out_proj.bias : tensor<65xbf16>
    %11 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.0.self_attn.out_proj.bias : tensor<65xbf16> -> !torch.vtensor<[65],bf16>
    %__auto.text_model.encoder.layers.0.layer_norm2.weight = util.global.load @__auto.text_model.encoder.layers.0.layer_norm2.weight : tensor<65xbf16>
    %12 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.0.layer_norm2.weight : tensor<65xbf16> -> !torch.vtensor<[65],bf16>
    %__auto.text_model.encoder.layers.0.layer_norm2.bias = util.global.load @__auto.text_model.encoder.layers.0.layer_norm2.bias : tensor<65xbf16>
    %13 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.0.layer_norm2.bias : tensor<65xbf16> -> !torch.vtensor<[65],bf16>
    %__auto.text_model.encoder.layers.0.mlp.fc1.weight = util.global.load @__auto.text_model.encoder.layers.0.mlp.fc1.weight : tensor<7x65xbf16>
    %14 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.0.mlp.fc1.weight : tensor<7x65xbf16> -> !torch.vtensor<[7,65],bf16>
    %__auto.text_model.encoder.layers.0.mlp.fc1.bias = util.global.load @__auto.text_model.encoder.layers.0.mlp.fc1.bias : tensor<7xbf16>
    %15 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.0.mlp.fc1.bias : tensor<7xbf16> -> !torch.vtensor<[7],bf16>
    %__auto.text_model.encoder.layers.0.mlp.fc2.weight = util.global.load @__auto.text_model.encoder.layers.0.mlp.fc2.weight : tensor<65x7xbf16>
    %16 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.0.mlp.fc2.weight : tensor<65x7xbf16> -> !torch.vtensor<[65,7],bf16>
    %__auto.text_model.encoder.layers.0.mlp.fc2.bias = util.global.load @__auto.text_model.encoder.layers.0.mlp.fc2.bias : tensor<65xbf16>
    %17 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.0.mlp.fc2.bias : tensor<65xbf16> -> !torch.vtensor<[65],bf16>
    %__auto.text_model.encoder.layers.1.layer_norm1.weight = util.global.load @__auto.text_model.encoder.layers.1.layer_norm1.weight : tensor<65xbf16>
    %18 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.1.layer_norm1.weight : tensor<65xbf16> -> !torch.vtensor<[65],bf16>
    %__auto.text_model.encoder.layers.1.layer_norm1.bias = util.global.load @__auto.text_model.encoder.layers.1.layer_norm1.bias : tensor<65xbf16>
    %19 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.1.layer_norm1.bias : tensor<65xbf16> -> !torch.vtensor<[65],bf16>
    %__auto.text_model.encoder.layers.1.self_attn.q_proj.weight = util.global.load @__auto.text_model.encoder.layers.1.self_attn.q_proj.weight : tensor<65x65xbf16>
    %20 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.1.self_attn.q_proj.weight : tensor<65x65xbf16> -> !torch.vtensor<[65,65],bf16>
    %__auto.text_model.encoder.layers.1.self_attn.q_proj.bias = util.global.load @__auto.text_model.encoder.layers.1.self_attn.q_proj.bias : tensor<65xbf16>
    %21 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.1.self_attn.q_proj.bias : tensor<65xbf16> -> !torch.vtensor<[65],bf16>
    %__auto.text_model.encoder.layers.1.self_attn.k_proj.weight = util.global.load @__auto.text_model.encoder.layers.1.self_attn.k_proj.weight : tensor<65x65xbf16>
    %22 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.1.self_attn.k_proj.weight : tensor<65x65xbf16> -> !torch.vtensor<[65,65],bf16>
    %__auto.text_model.encoder.layers.1.self_attn.k_proj.bias = util.global.load @__auto.text_model.encoder.layers.1.self_attn.k_proj.bias : tensor<65xbf16>
    %23 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.1.self_attn.k_proj.bias : tensor<65xbf16> -> !torch.vtensor<[65],bf16>
    %__auto.text_model.encoder.layers.1.self_attn.v_proj.weight = util.global.load @__auto.text_model.encoder.layers.1.self_attn.v_proj.weight : tensor<65x65xbf16>
    %24 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.1.self_attn.v_proj.weight : tensor<65x65xbf16> -> !torch.vtensor<[65,65],bf16>
    %__auto.text_model.encoder.layers.1.self_attn.v_proj.bias = util.global.load @__auto.text_model.encoder.layers.1.self_attn.v_proj.bias : tensor<65xbf16>
    %25 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.1.self_attn.v_proj.bias : tensor<65xbf16> -> !torch.vtensor<[65],bf16>
    %__auto.text_model.encoder.layers.1.self_attn.out_proj.weight = util.global.load @__auto.text_model.encoder.layers.1.self_attn.out_proj.weight : tensor<65x65xbf16>
    %26 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.1.self_attn.out_proj.weight : tensor<65x65xbf16> -> !torch.vtensor<[65,65],bf16>
    %__auto.text_model.encoder.layers.1.self_attn.out_proj.bias = util.global.load @__auto.text_model.encoder.layers.1.self_attn.out_proj.bias : tensor<65xbf16>
    %27 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.1.self_attn.out_proj.bias : tensor<65xbf16> -> !torch.vtensor<[65],bf16>
    %__auto.text_model.encoder.layers.1.layer_norm2.weight = util.global.load @__auto.text_model.encoder.layers.1.layer_norm2.weight : tensor<65xbf16>
    %28 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.1.layer_norm2.weight : tensor<65xbf16> -> !torch.vtensor<[65],bf16>
    %__auto.text_model.encoder.layers.1.layer_norm2.bias = util.global.load @__auto.text_model.encoder.layers.1.layer_norm2.bias : tensor<65xbf16>
    %29 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.1.layer_norm2.bias : tensor<65xbf16> -> !torch.vtensor<[65],bf16>
    %__auto.text_model.encoder.layers.1.mlp.fc1.weight = util.global.load @__auto.text_model.encoder.layers.1.mlp.fc1.weight : tensor<7x65xbf16>
    %30 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.1.mlp.fc1.weight : tensor<7x65xbf16> -> !torch.vtensor<[7,65],bf16>
    %__auto.text_model.encoder.layers.1.mlp.fc1.bias = util.global.load @__auto.text_model.encoder.layers.1.mlp.fc1.bias : tensor<7xbf16>
    %31 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.1.mlp.fc1.bias : tensor<7xbf16> -> !torch.vtensor<[7],bf16>
    %__auto.text_model.encoder.layers.1.mlp.fc2.weight = util.global.load @__auto.text_model.encoder.layers.1.mlp.fc2.weight : tensor<65x7xbf16>
    %32 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.1.mlp.fc2.weight : tensor<65x7xbf16> -> !torch.vtensor<[65,7],bf16>
    %__auto.text_model.encoder.layers.1.mlp.fc2.bias = util.global.load @__auto.text_model.encoder.layers.1.mlp.fc2.bias : tensor<65xbf16>
    %33 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.1.mlp.fc2.bias : tensor<65xbf16> -> !torch.vtensor<[65],bf16>
    %__auto.text_model.final_layer_norm.weight = util.global.load @__auto.text_model.final_layer_norm.weight : tensor<65xbf16>
    %34 = torch_c.from_builtin_tensor %__auto.text_model.final_layer_norm.weight : tensor<65xbf16> -> !torch.vtensor<[65],bf16>
    %__auto.text_model.final_layer_norm.bias = util.global.load @__auto.text_model.final_layer_norm.bias : tensor<65xbf16>
    %35 = torch_c.from_builtin_tensor %__auto.text_model.final_layer_norm.bias : tensor<65xbf16> -> !torch.vtensor<[65],bf16>
    %36 = torch.symbolic_int "s0" {min_val = 2, max_val = 9223372036854775806} : !torch.int
    %int-1 = torch.constant.int -1
    %int17 = torch.constant.int 17
    %37 = torch.prim.ListConstruct %int-1, %int17 : (!torch.int, !torch.int) -> !torch.list<int>
    %38 = torch.aten.view %arg0, %37 : !torch.vtensor<[4,17],si64>, !torch.list<int> -> !torch.vtensor<[4,17],si64>
    %39 = torch.vtensor.literal(dense_resource<torch_tensor_1_17_torch.int64> : tensor<1x17xsi64>) : !torch.vtensor<[1,17],si64>
    %int0 = torch.constant.int 0
    %int0_0 = torch.constant.int 0
    %int9223372036854775807 = torch.constant.int 9223372036854775807
    %int1 = torch.constant.int 1
    %40 = torch.aten.slice.Tensor %39, %int0, %int0_0, %int9223372036854775807, %int1 : !torch.vtensor<[1,17],si64>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,17],si64>
    %int-1_1 = torch.constant.int -1
    %false = torch.constant.bool false
    %false_2 = torch.constant.bool false
    %41 = torch.aten.embedding %0, %38, %int-1_1, %false, %false_2 : !torch.vtensor<[11,65],bf16>, !torch.vtensor<[4,17],si64>, !torch.int, !torch.bool, !torch.bool -> !torch.vtensor<[4,17,65],bf16>
    %int-1_3 = torch.constant.int -1
    %false_4 = torch.constant.bool false
    %false_5 = torch.constant.bool false
    %42 = torch.aten.embedding %1, %40, %int-1_3, %false_4, %false_5 : !torch.vtensor<[17,65],bf16>, !torch.vtensor<[1,17],si64>, !torch.int, !torch.bool, !torch.bool -> !torch.vtensor<[1,17,65],bf16>
    %int1_6 = torch.constant.int 1
    %43 = torch.aten.add.Tensor %41, %42, %int1_6 : !torch.vtensor<[4,17,65],bf16>, !torch.vtensor<[1,17,65],bf16>, !torch.int -> !torch.vtensor<[4,17,65],bf16>
    %int17_7 = torch.constant.int 17
    %int17_8 = torch.constant.int 17
    %44 = torch.prim.ListConstruct %int17_7, %int17_8 : (!torch.int, !torch.int) -> !torch.list<int>
    %int17_9 = torch.constant.int 17
    %int1_10 = torch.constant.int 1
    %45 = torch.prim.ListConstruct %int17_9, %int1_10 : (!torch.int, !torch.int) -> !torch.list<int>
    %int6 = torch.constant.int 6
    %int0_11 = torch.constant.int 0
    %cpu = torch.constant.device "cpu"
    %false_12 = torch.constant.bool false
    %46 = torch.aten.empty_strided %44, %45, %int6, %int0_11, %cpu, %false_12 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[17,17],f32>
    %float-3.389530e38 = torch.constant.float -3.3895313892515355E+38
    %47 = torch.aten.fill.Scalar %46, %float-3.389530e38 : !torch.vtensor<[17,17],f32>, !torch.float -> !torch.vtensor<[17,17],f32>
    %int17_13 = torch.constant.int 17
    %none = torch.constant.none
    %none_14 = torch.constant.none
    %cpu_15 = torch.constant.device "cpu"
    %false_16 = torch.constant.bool false
    %48 = torch.aten.arange %int17_13, %none, %none_14, %cpu_15, %false_16 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[17],si64>
    %int1_17 = torch.constant.int 1
    %int1_18 = torch.constant.int 1
    %49 = torch.aten.add.Scalar %48, %int1_17, %int1_18 : !torch.vtensor<[17],si64>, !torch.int, !torch.int -> !torch.vtensor<[17],si64>
    %int17_19 = torch.constant.int 17
    %int1_20 = torch.constant.int 1
    %50 = torch.prim.ListConstruct %int17_19, %int1_20 : (!torch.int, !torch.int) -> !torch.list<int>
    %51 = torch.aten.view %49, %50 : !torch.vtensor<[17],si64>, !torch.list<int> -> !torch.vtensor<[17,1],si64>
    %52 = torch.aten.lt.Tensor %48, %51 : !torch.vtensor<[17],si64>, !torch.vtensor<[17,1],si64> -> !torch.vtensor<[17,17],i1>
    %float0.000000e00 = torch.constant.float 0.000000e+00
    %int6_21 = torch.constant.int 6
    %int0_22 = torch.constant.int 0
    %cpu_23 = torch.constant.device "cpu"
    %none_24 = torch.constant.none
    %53 = torch.aten.scalar_tensor %float0.000000e00, %int6_21, %int0_22, %cpu_23, %none_24 : !torch.float, !torch.int, !torch.int, !torch.Device, !torch.none -> !torch.vtensor<[],f32>
    %54 = torch.aten.where.self %52, %53, %47 : !torch.vtensor<[17,17],i1>, !torch.vtensor<[],f32>, !torch.vtensor<[17,17],f32> -> !torch.vtensor<[17,17],f32>
    %int15 = torch.constant.int 15
    %55 = torch.prims.convert_element_type %54, %int15 : !torch.vtensor<[17,17],f32>, !torch.int -> !torch.vtensor<[17,17],bf16>
    %int0_25 = torch.constant.int 0
    %56 = torch.aten.unsqueeze %55, %int0_25 : !torch.vtensor<[17,17],bf16>, !torch.int -> !torch.vtensor<[1,17,17],bf16>
    %int1_26 = torch.constant.int 1
    %57 = torch.aten.unsqueeze %56, %int1_26 : !torch.vtensor<[1,17,17],bf16>, !torch.int -> !torch.vtensor<[1,1,17,17],bf16>
    %int2 = torch.constant.int 2
    %int0_27 = torch.constant.int 0
    %int9223372036854775807_28 = torch.constant.int 9223372036854775807
    %int1_29 = torch.constant.int 1
    %58 = torch.aten.slice.Tensor %57, %int2, %int0_27, %int9223372036854775807_28, %int1_29 : !torch.vtensor<[1,1,17,17],bf16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,1,17,17],bf16>
    %int3 = torch.constant.int 3
    %int0_30 = torch.constant.int 0
    %int9223372036854775807_31 = torch.constant.int 9223372036854775807
    %int1_32 = torch.constant.int 1
    %59 = torch.aten.slice.Tensor %58, %int3, %int0_30, %int9223372036854775807_31, %int1_32 : !torch.vtensor<[1,1,17,17],bf16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,1,17,17],bf16>
    %int4 = torch.constant.int 4
    %int1_33 = torch.constant.int 1
    %int17_34 = torch.constant.int 17
    %int17_35 = torch.constant.int 17
    %60 = torch.prim.ListConstruct %int4, %int1_33, %int17_34, %int17_35 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_36 = torch.constant.bool false
    %61 = torch.aten.expand %59, %60, %false_36 : !torch.vtensor<[1,1,17,17],bf16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,1,17,17],bf16>
    %int6_37 = torch.constant.int 6
    %62 = torch.prims.convert_element_type %43, %int6_37 : !torch.vtensor<[4,17,65],bf16>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %int2_38 = torch.constant.int 2
    %63 = torch.prim.ListConstruct %int2_38 : (!torch.int) -> !torch.list<int>
    %int0_39 = torch.constant.int 0
    %true = torch.constant.bool true
    %result0, %result1 = torch.aten.var_mean.correction %62, %63, %int0_39, %true : !torch.vtensor<[4,17,65],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[4,17,1],f32>, !torch.vtensor<[4,17,1],f32>
    %float1.000000e-04 = torch.constant.float 1.000000e-04
    %int1_40 = torch.constant.int 1
    %64 = torch.aten.add.Scalar %result0, %float1.000000e-04, %int1_40 : !torch.vtensor<[4,17,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,17,1],f32>
    %65 = torch.aten.rsqrt %64 : !torch.vtensor<[4,17,1],f32> -> !torch.vtensor<[4,17,1],f32>
    %int1_41 = torch.constant.int 1
    %66 = torch.aten.sub.Tensor %43, %result1, %int1_41 : !torch.vtensor<[4,17,65],bf16>, !torch.vtensor<[4,17,1],f32>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %67 = torch.aten.mul.Tensor %66, %65 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[4,17,1],f32> -> !torch.vtensor<[4,17,65],f32>
    %68 = torch.aten.mul.Tensor %67, %2 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[65],bf16> -> !torch.vtensor<[4,17,65],f32>
    %int1_42 = torch.constant.int 1
    %69 = torch.aten.add.Tensor %68, %3, %int1_42 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[65],bf16>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %int15_43 = torch.constant.int 15
    %70 = torch.prims.convert_element_type %69, %int15_43 : !torch.vtensor<[4,17,65],f32>, !torch.int -> !torch.vtensor<[4,17,65],bf16>
    %int-2 = torch.constant.int -2
    %int-1_44 = torch.constant.int -1
    %71 = torch.aten.transpose.int %4, %int-2, %int-1_44 : !torch.vtensor<[65,65],bf16>, !torch.int, !torch.int -> !torch.vtensor<[65,65],bf16>
    %int68 = torch.constant.int 68
    %int65 = torch.constant.int 65
    %72 = torch.prim.ListConstruct %int68, %int65 : (!torch.int, !torch.int) -> !torch.list<int>
    %73 = torch.aten.view %70, %72 : !torch.vtensor<[4,17,65],bf16>, !torch.list<int> -> !torch.vtensor<[68,65],bf16>
    %74 = torch.aten.mm %73, %71 : !torch.vtensor<[68,65],bf16>, !torch.vtensor<[65,65],bf16> -> !torch.vtensor<[68,65],bf16>
    %int4_45 = torch.constant.int 4
    %int17_46 = torch.constant.int 17
    %int65_47 = torch.constant.int 65
    %75 = torch.prim.ListConstruct %int4_45, %int17_46, %int65_47 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %76 = torch.aten.view %74, %75 : !torch.vtensor<[68,65],bf16>, !torch.list<int> -> !torch.vtensor<[4,17,65],bf16>
    %int1_48 = torch.constant.int 1
    %77 = torch.aten.add.Tensor %76, %5, %int1_48 : !torch.vtensor<[4,17,65],bf16>, !torch.vtensor<[65],bf16>, !torch.int -> !torch.vtensor<[4,17,65],bf16>
    %float2.773500e-01 = torch.constant.float 0.27735009811261457
    %78 = torch.aten.mul.Scalar %77, %float2.773500e-01 : !torch.vtensor<[4,17,65],bf16>, !torch.float -> !torch.vtensor<[4,17,65],bf16>
    %int-2_49 = torch.constant.int -2
    %int-1_50 = torch.constant.int -1
    %79 = torch.aten.transpose.int %6, %int-2_49, %int-1_50 : !torch.vtensor<[65,65],bf16>, !torch.int, !torch.int -> !torch.vtensor<[65,65],bf16>
    %int68_51 = torch.constant.int 68
    %int65_52 = torch.constant.int 65
    %80 = torch.prim.ListConstruct %int68_51, %int65_52 : (!torch.int, !torch.int) -> !torch.list<int>
    %81 = torch.aten.view %70, %80 : !torch.vtensor<[4,17,65],bf16>, !torch.list<int> -> !torch.vtensor<[68,65],bf16>
    %82 = torch.aten.mm %81, %79 : !torch.vtensor<[68,65],bf16>, !torch.vtensor<[65,65],bf16> -> !torch.vtensor<[68,65],bf16>
    %int4_53 = torch.constant.int 4
    %int17_54 = torch.constant.int 17
    %int65_55 = torch.constant.int 65
    %83 = torch.prim.ListConstruct %int4_53, %int17_54, %int65_55 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %84 = torch.aten.view %82, %83 : !torch.vtensor<[68,65],bf16>, !torch.list<int> -> !torch.vtensor<[4,17,65],bf16>
    %int1_56 = torch.constant.int 1
    %85 = torch.aten.add.Tensor %84, %7, %int1_56 : !torch.vtensor<[4,17,65],bf16>, !torch.vtensor<[65],bf16>, !torch.int -> !torch.vtensor<[4,17,65],bf16>
    %int4_57 = torch.constant.int 4
    %int-1_58 = torch.constant.int -1
    %int5 = torch.constant.int 5
    %int13 = torch.constant.int 13
    %86 = torch.prim.ListConstruct %int4_57, %int-1_58, %int5, %int13 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %87 = torch.aten.view %85, %86 : !torch.vtensor<[4,17,65],bf16>, !torch.list<int> -> !torch.vtensor<[4,17,5,13],bf16>
    %int1_59 = torch.constant.int 1
    %int2_60 = torch.constant.int 2
    %88 = torch.aten.transpose.int %87, %int1_59, %int2_60 : !torch.vtensor<[4,17,5,13],bf16>, !torch.int, !torch.int -> !torch.vtensor<[4,5,17,13],bf16>
    %int0_61 = torch.constant.int 0
    %89 = torch.aten.clone %88, %int0_61 : !torch.vtensor<[4,5,17,13],bf16>, !torch.int -> !torch.vtensor<[4,5,17,13],bf16>
    %int-2_62 = torch.constant.int -2
    %int-1_63 = torch.constant.int -1
    %90 = torch.aten.transpose.int %8, %int-2_62, %int-1_63 : !torch.vtensor<[65,65],bf16>, !torch.int, !torch.int -> !torch.vtensor<[65,65],bf16>
    %int68_64 = torch.constant.int 68
    %int65_65 = torch.constant.int 65
    %91 = torch.prim.ListConstruct %int68_64, %int65_65 : (!torch.int, !torch.int) -> !torch.list<int>
    %92 = torch.aten.view %70, %91 : !torch.vtensor<[4,17,65],bf16>, !torch.list<int> -> !torch.vtensor<[68,65],bf16>
    %93 = torch.aten.mm %92, %90 : !torch.vtensor<[68,65],bf16>, !torch.vtensor<[65,65],bf16> -> !torch.vtensor<[68,65],bf16>
    %int4_66 = torch.constant.int 4
    %int17_67 = torch.constant.int 17
    %int65_68 = torch.constant.int 65
    %94 = torch.prim.ListConstruct %int4_66, %int17_67, %int65_68 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %95 = torch.aten.view %93, %94 : !torch.vtensor<[68,65],bf16>, !torch.list<int> -> !torch.vtensor<[4,17,65],bf16>
    %int1_69 = torch.constant.int 1
    %96 = torch.aten.add.Tensor %95, %9, %int1_69 : !torch.vtensor<[4,17,65],bf16>, !torch.vtensor<[65],bf16>, !torch.int -> !torch.vtensor<[4,17,65],bf16>
    %int4_70 = torch.constant.int 4
    %int-1_71 = torch.constant.int -1
    %int5_72 = torch.constant.int 5
    %int13_73 = torch.constant.int 13
    %97 = torch.prim.ListConstruct %int4_70, %int-1_71, %int5_72, %int13_73 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %98 = torch.aten.view %96, %97 : !torch.vtensor<[4,17,65],bf16>, !torch.list<int> -> !torch.vtensor<[4,17,5,13],bf16>
    %int1_74 = torch.constant.int 1
    %int2_75 = torch.constant.int 2
    %99 = torch.aten.transpose.int %98, %int1_74, %int2_75 : !torch.vtensor<[4,17,5,13],bf16>, !torch.int, !torch.int -> !torch.vtensor<[4,5,17,13],bf16>
    %int0_76 = torch.constant.int 0
    %100 = torch.aten.clone %99, %int0_76 : !torch.vtensor<[4,5,17,13],bf16>, !torch.int -> !torch.vtensor<[4,5,17,13],bf16>
    %int4_77 = torch.constant.int 4
    %int17_78 = torch.constant.int 17
    %int5_79 = torch.constant.int 5
    %int13_80 = torch.constant.int 13
    %101 = torch.prim.ListConstruct %int4_77, %int17_78, %int5_79, %int13_80 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %102 = torch.aten.view %78, %101 : !torch.vtensor<[4,17,65],bf16>, !torch.list<int> -> !torch.vtensor<[4,17,5,13],bf16>
    %int1_81 = torch.constant.int 1
    %int2_82 = torch.constant.int 2
    %103 = torch.aten.transpose.int %102, %int1_81, %int2_82 : !torch.vtensor<[4,17,5,13],bf16>, !torch.int, !torch.int -> !torch.vtensor<[4,5,17,13],bf16>
    %int0_83 = torch.constant.int 0
    %104 = torch.aten.clone %103, %int0_83 : !torch.vtensor<[4,5,17,13],bf16>, !torch.int -> !torch.vtensor<[4,5,17,13],bf16>
    %int20 = torch.constant.int 20
    %int-1_84 = torch.constant.int -1
    %int13_85 = torch.constant.int 13
    %105 = torch.prim.ListConstruct %int20, %int-1_84, %int13_85 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %106 = torch.aten.view %104, %105 : !torch.vtensor<[4,5,17,13],bf16>, !torch.list<int> -> !torch.vtensor<[20,17,13],bf16>
    %int20_86 = torch.constant.int 20
    %int-1_87 = torch.constant.int -1
    %int13_88 = torch.constant.int 13
    %107 = torch.prim.ListConstruct %int20_86, %int-1_87, %int13_88 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %108 = torch.aten.view %89, %107 : !torch.vtensor<[4,5,17,13],bf16>, !torch.list<int> -> !torch.vtensor<[20,17,13],bf16>
    %int20_89 = torch.constant.int 20
    %int-1_90 = torch.constant.int -1
    %int13_91 = torch.constant.int 13
    %109 = torch.prim.ListConstruct %int20_89, %int-1_90, %int13_91 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %110 = torch.aten.view %100, %109 : !torch.vtensor<[4,5,17,13],bf16>, !torch.list<int> -> !torch.vtensor<[20,17,13],bf16>
    %int1_92 = torch.constant.int 1
    %int2_93 = torch.constant.int 2
    %111 = torch.aten.transpose.int %108, %int1_92, %int2_93 : !torch.vtensor<[20,17,13],bf16>, !torch.int, !torch.int -> !torch.vtensor<[20,13,17],bf16>
    %int20_94 = torch.constant.int 20
    %int17_95 = torch.constant.int 17
    %int13_96 = torch.constant.int 13
    %112 = torch.prim.ListConstruct %int20_94, %int17_95, %int13_96 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_97 = torch.constant.bool false
    %113 = torch.aten.expand %106, %112, %false_97 : !torch.vtensor<[20,17,13],bf16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[20,17,13],bf16>
    %int20_98 = torch.constant.int 20
    %int17_99 = torch.constant.int 17
    %int13_100 = torch.constant.int 13
    %114 = torch.prim.ListConstruct %int20_98, %int17_99, %int13_100 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %115 = torch.aten.view %113, %114 : !torch.vtensor<[20,17,13],bf16>, !torch.list<int> -> !torch.vtensor<[20,17,13],bf16>
    %int20_101 = torch.constant.int 20
    %int13_102 = torch.constant.int 13
    %int17_103 = torch.constant.int 17
    %116 = torch.prim.ListConstruct %int20_101, %int13_102, %int17_103 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_104 = torch.constant.bool false
    %117 = torch.aten.expand %111, %116, %false_104 : !torch.vtensor<[20,13,17],bf16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[20,13,17],bf16>
    %int20_105 = torch.constant.int 20
    %int13_106 = torch.constant.int 13
    %int17_107 = torch.constant.int 17
    %118 = torch.prim.ListConstruct %int20_105, %int13_106, %int17_107 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %119 = torch.aten.view %117, %118 : !torch.vtensor<[20,13,17],bf16>, !torch.list<int> -> !torch.vtensor<[20,13,17],bf16>
    %120 = torch.aten.bmm %115, %119 : !torch.vtensor<[20,17,13],bf16>, !torch.vtensor<[20,13,17],bf16> -> !torch.vtensor<[20,17,17],bf16>
    %int20_108 = torch.constant.int 20
    %int17_109 = torch.constant.int 17
    %int17_110 = torch.constant.int 17
    %121 = torch.prim.ListConstruct %int20_108, %int17_109, %int17_110 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %122 = torch.aten.view %120, %121 : !torch.vtensor<[20,17,17],bf16>, !torch.list<int> -> !torch.vtensor<[20,17,17],bf16>
    %int4_111 = torch.constant.int 4
    %int5_112 = torch.constant.int 5
    %int17_113 = torch.constant.int 17
    %int17_114 = torch.constant.int 17
    %123 = torch.prim.ListConstruct %int4_111, %int5_112, %int17_113, %int17_114 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %124 = torch.aten.view %122, %123 : !torch.vtensor<[20,17,17],bf16>, !torch.list<int> -> !torch.vtensor<[4,5,17,17],bf16>
    %int1_115 = torch.constant.int 1
    %125 = torch.aten.add.Tensor %124, %61, %int1_115 : !torch.vtensor<[4,5,17,17],bf16>, !torch.vtensor<[4,1,17,17],bf16>, !torch.int -> !torch.vtensor<[4,5,17,17],bf16>
    %int20_116 = torch.constant.int 20
    %int17_117 = torch.constant.int 17
    %int17_118 = torch.constant.int 17
    %126 = torch.prim.ListConstruct %int20_116, %int17_117, %int17_118 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %127 = torch.aten.view %125, %126 : !torch.vtensor<[4,5,17,17],bf16>, !torch.list<int> -> !torch.vtensor<[20,17,17],bf16>
    %int-1_119 = torch.constant.int -1
    %false_120 = torch.constant.bool false
    %128 = torch.aten._softmax %127, %int-1_119, %false_120 : !torch.vtensor<[20,17,17],bf16>, !torch.int, !torch.bool -> !torch.vtensor<[20,17,17],bf16>
    %int20_121 = torch.constant.int 20
    %int17_122 = torch.constant.int 17
    %int17_123 = torch.constant.int 17
    %129 = torch.prim.ListConstruct %int20_121, %int17_122, %int17_123 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_124 = torch.constant.bool false
    %130 = torch.aten.expand %128, %129, %false_124 : !torch.vtensor<[20,17,17],bf16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[20,17,17],bf16>
    %int20_125 = torch.constant.int 20
    %int17_126 = torch.constant.int 17
    %int17_127 = torch.constant.int 17
    %131 = torch.prim.ListConstruct %int20_125, %int17_126, %int17_127 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %132 = torch.aten.view %130, %131 : !torch.vtensor<[20,17,17],bf16>, !torch.list<int> -> !torch.vtensor<[20,17,17],bf16>
    %int20_128 = torch.constant.int 20
    %int17_129 = torch.constant.int 17
    %int13_130 = torch.constant.int 13
    %133 = torch.prim.ListConstruct %int20_128, %int17_129, %int13_130 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_131 = torch.constant.bool false
    %134 = torch.aten.expand %110, %133, %false_131 : !torch.vtensor<[20,17,13],bf16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[20,17,13],bf16>
    %int20_132 = torch.constant.int 20
    %int17_133 = torch.constant.int 17
    %int13_134 = torch.constant.int 13
    %135 = torch.prim.ListConstruct %int20_132, %int17_133, %int13_134 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %136 = torch.aten.view %134, %135 : !torch.vtensor<[20,17,13],bf16>, !torch.list<int> -> !torch.vtensor<[20,17,13],bf16>
    %137 = torch.aten.bmm %132, %136 : !torch.vtensor<[20,17,17],bf16>, !torch.vtensor<[20,17,13],bf16> -> !torch.vtensor<[20,17,13],bf16>
    %int20_135 = torch.constant.int 20
    %int17_136 = torch.constant.int 17
    %int13_137 = torch.constant.int 13
    %138 = torch.prim.ListConstruct %int20_135, %int17_136, %int13_137 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %139 = torch.aten.view %137, %138 : !torch.vtensor<[20,17,13],bf16>, !torch.list<int> -> !torch.vtensor<[20,17,13],bf16>
    %int4_138 = torch.constant.int 4
    %int5_139 = torch.constant.int 5
    %int17_140 = torch.constant.int 17
    %int13_141 = torch.constant.int 13
    %140 = torch.prim.ListConstruct %int4_138, %int5_139, %int17_140, %int13_141 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %141 = torch.aten.view %139, %140 : !torch.vtensor<[20,17,13],bf16>, !torch.list<int> -> !torch.vtensor<[4,5,17,13],bf16>
    %int1_142 = torch.constant.int 1
    %int2_143 = torch.constant.int 2
    %142 = torch.aten.transpose.int %141, %int1_142, %int2_143 : !torch.vtensor<[4,5,17,13],bf16>, !torch.int, !torch.int -> !torch.vtensor<[4,17,5,13],bf16>
    %int0_144 = torch.constant.int 0
    %143 = torch.aten.clone %142, %int0_144 : !torch.vtensor<[4,17,5,13],bf16>, !torch.int -> !torch.vtensor<[4,17,5,13],bf16>
    %int4_145 = torch.constant.int 4
    %int17_146 = torch.constant.int 17
    %int65_147 = torch.constant.int 65
    %144 = torch.prim.ListConstruct %int4_145, %int17_146, %int65_147 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %145 = torch.aten._unsafe_view %143, %144 : !torch.vtensor<[4,17,5,13],bf16>, !torch.list<int> -> !torch.vtensor<[4,17,65],bf16>
    %int-2_148 = torch.constant.int -2
    %int-1_149 = torch.constant.int -1
    %146 = torch.aten.transpose.int %10, %int-2_148, %int-1_149 : !torch.vtensor<[65,65],bf16>, !torch.int, !torch.int -> !torch.vtensor<[65,65],bf16>
    %int68_150 = torch.constant.int 68
    %int65_151 = torch.constant.int 65
    %147 = torch.prim.ListConstruct %int68_150, %int65_151 : (!torch.int, !torch.int) -> !torch.list<int>
    %148 = torch.aten.view %145, %147 : !torch.vtensor<[4,17,65],bf16>, !torch.list<int> -> !torch.vtensor<[68,65],bf16>
    %149 = torch.aten.mm %148, %146 : !torch.vtensor<[68,65],bf16>, !torch.vtensor<[65,65],bf16> -> !torch.vtensor<[68,65],bf16>
    %int4_152 = torch.constant.int 4
    %int17_153 = torch.constant.int 17
    %int65_154 = torch.constant.int 65
    %150 = torch.prim.ListConstruct %int4_152, %int17_153, %int65_154 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %151 = torch.aten.view %149, %150 : !torch.vtensor<[68,65],bf16>, !torch.list<int> -> !torch.vtensor<[4,17,65],bf16>
    %int1_155 = torch.constant.int 1
    %152 = torch.aten.add.Tensor %151, %11, %int1_155 : !torch.vtensor<[4,17,65],bf16>, !torch.vtensor<[65],bf16>, !torch.int -> !torch.vtensor<[4,17,65],bf16>
    %int1_156 = torch.constant.int 1
    %153 = torch.aten.add.Tensor %43, %152, %int1_156 : !torch.vtensor<[4,17,65],bf16>, !torch.vtensor<[4,17,65],bf16>, !torch.int -> !torch.vtensor<[4,17,65],bf16>
    %int6_157 = torch.constant.int 6
    %154 = torch.prims.convert_element_type %153, %int6_157 : !torch.vtensor<[4,17,65],bf16>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %int2_158 = torch.constant.int 2
    %155 = torch.prim.ListConstruct %int2_158 : (!torch.int) -> !torch.list<int>
    %int0_159 = torch.constant.int 0
    %true_160 = torch.constant.bool true
    %result0_161, %result1_162 = torch.aten.var_mean.correction %154, %155, %int0_159, %true_160 : !torch.vtensor<[4,17,65],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[4,17,1],f32>, !torch.vtensor<[4,17,1],f32>
    %float1.000000e-04_163 = torch.constant.float 1.000000e-04
    %int1_164 = torch.constant.int 1
    %156 = torch.aten.add.Scalar %result0_161, %float1.000000e-04_163, %int1_164 : !torch.vtensor<[4,17,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,17,1],f32>
    %157 = torch.aten.rsqrt %156 : !torch.vtensor<[4,17,1],f32> -> !torch.vtensor<[4,17,1],f32>
    %int1_165 = torch.constant.int 1
    %158 = torch.aten.sub.Tensor %153, %result1_162, %int1_165 : !torch.vtensor<[4,17,65],bf16>, !torch.vtensor<[4,17,1],f32>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %159 = torch.aten.mul.Tensor %158, %157 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[4,17,1],f32> -> !torch.vtensor<[4,17,65],f32>
    %160 = torch.aten.mul.Tensor %159, %12 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[65],bf16> -> !torch.vtensor<[4,17,65],f32>
    %int1_166 = torch.constant.int 1
    %161 = torch.aten.add.Tensor %160, %13, %int1_166 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[65],bf16>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %int15_167 = torch.constant.int 15
    %162 = torch.prims.convert_element_type %161, %int15_167 : !torch.vtensor<[4,17,65],f32>, !torch.int -> !torch.vtensor<[4,17,65],bf16>
    %int-2_168 = torch.constant.int -2
    %int-1_169 = torch.constant.int -1
    %163 = torch.aten.transpose.int %14, %int-2_168, %int-1_169 : !torch.vtensor<[7,65],bf16>, !torch.int, !torch.int -> !torch.vtensor<[65,7],bf16>
    %int68_170 = torch.constant.int 68
    %int65_171 = torch.constant.int 65
    %164 = torch.prim.ListConstruct %int68_170, %int65_171 : (!torch.int, !torch.int) -> !torch.list<int>
    %165 = torch.aten.view %162, %164 : !torch.vtensor<[4,17,65],bf16>, !torch.list<int> -> !torch.vtensor<[68,65],bf16>
    %166 = torch.aten.mm %165, %163 : !torch.vtensor<[68,65],bf16>, !torch.vtensor<[65,7],bf16> -> !torch.vtensor<[68,7],bf16>
    %int4_172 = torch.constant.int 4
    %int17_173 = torch.constant.int 17
    %int7 = torch.constant.int 7
    %167 = torch.prim.ListConstruct %int4_172, %int17_173, %int7 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %168 = torch.aten.view %166, %167 : !torch.vtensor<[68,7],bf16>, !torch.list<int> -> !torch.vtensor<[4,17,7],bf16>
    %int1_174 = torch.constant.int 1
    %169 = torch.aten.add.Tensor %168, %15, %int1_174 : !torch.vtensor<[4,17,7],bf16>, !torch.vtensor<[7],bf16>, !torch.int -> !torch.vtensor<[4,17,7],bf16>
    %float1.702000e00 = torch.constant.float 1.702000e+00
    %170 = torch.aten.mul.Scalar %169, %float1.702000e00 : !torch.vtensor<[4,17,7],bf16>, !torch.float -> !torch.vtensor<[4,17,7],bf16>
    %171 = torch.aten.sigmoid %170 : !torch.vtensor<[4,17,7],bf16> -> !torch.vtensor<[4,17,7],bf16>
    %172 = torch.aten.mul.Tensor %169, %171 : !torch.vtensor<[4,17,7],bf16>, !torch.vtensor<[4,17,7],bf16> -> !torch.vtensor<[4,17,7],bf16>
    %int-2_175 = torch.constant.int -2
    %int-1_176 = torch.constant.int -1
    %173 = torch.aten.transpose.int %16, %int-2_175, %int-1_176 : !torch.vtensor<[65,7],bf16>, !torch.int, !torch.int -> !torch.vtensor<[7,65],bf16>
    %int68_177 = torch.constant.int 68
    %int7_178 = torch.constant.int 7
    %174 = torch.prim.ListConstruct %int68_177, %int7_178 : (!torch.int, !torch.int) -> !torch.list<int>
    %175 = torch.aten.view %172, %174 : !torch.vtensor<[4,17,7],bf16>, !torch.list<int> -> !torch.vtensor<[68,7],bf16>
    %176 = torch.aten.mm %175, %173 : !torch.vtensor<[68,7],bf16>, !torch.vtensor<[7,65],bf16> -> !torch.vtensor<[68,65],bf16>
    %int4_179 = torch.constant.int 4
    %int17_180 = torch.constant.int 17
    %int65_181 = torch.constant.int 65
    %177 = torch.prim.ListConstruct %int4_179, %int17_180, %int65_181 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %178 = torch.aten.view %176, %177 : !torch.vtensor<[68,65],bf16>, !torch.list<int> -> !torch.vtensor<[4,17,65],bf16>
    %int1_182 = torch.constant.int 1
    %179 = torch.aten.add.Tensor %178, %17, %int1_182 : !torch.vtensor<[4,17,65],bf16>, !torch.vtensor<[65],bf16>, !torch.int -> !torch.vtensor<[4,17,65],bf16>
    %int1_183 = torch.constant.int 1
    %180 = torch.aten.add.Tensor %153, %179, %int1_183 : !torch.vtensor<[4,17,65],bf16>, !torch.vtensor<[4,17,65],bf16>, !torch.int -> !torch.vtensor<[4,17,65],bf16>
    %int6_184 = torch.constant.int 6
    %181 = torch.prims.convert_element_type %180, %int6_184 : !torch.vtensor<[4,17,65],bf16>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %int2_185 = torch.constant.int 2
    %182 = torch.prim.ListConstruct %int2_185 : (!torch.int) -> !torch.list<int>
    %int0_186 = torch.constant.int 0
    %true_187 = torch.constant.bool true
    %result0_188, %result1_189 = torch.aten.var_mean.correction %181, %182, %int0_186, %true_187 : !torch.vtensor<[4,17,65],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[4,17,1],f32>, !torch.vtensor<[4,17,1],f32>
    %float1.000000e-04_190 = torch.constant.float 1.000000e-04
    %int1_191 = torch.constant.int 1
    %183 = torch.aten.add.Scalar %result0_188, %float1.000000e-04_190, %int1_191 : !torch.vtensor<[4,17,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,17,1],f32>
    %184 = torch.aten.rsqrt %183 : !torch.vtensor<[4,17,1],f32> -> !torch.vtensor<[4,17,1],f32>
    %int1_192 = torch.constant.int 1
    %185 = torch.aten.sub.Tensor %180, %result1_189, %int1_192 : !torch.vtensor<[4,17,65],bf16>, !torch.vtensor<[4,17,1],f32>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %186 = torch.aten.mul.Tensor %185, %184 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[4,17,1],f32> -> !torch.vtensor<[4,17,65],f32>
    %187 = torch.aten.mul.Tensor %186, %18 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[65],bf16> -> !torch.vtensor<[4,17,65],f32>
    %int1_193 = torch.constant.int 1
    %188 = torch.aten.add.Tensor %187, %19, %int1_193 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[65],bf16>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %int15_194 = torch.constant.int 15
    %189 = torch.prims.convert_element_type %188, %int15_194 : !torch.vtensor<[4,17,65],f32>, !torch.int -> !torch.vtensor<[4,17,65],bf16>
    %int-2_195 = torch.constant.int -2
    %int-1_196 = torch.constant.int -1
    %190 = torch.aten.transpose.int %20, %int-2_195, %int-1_196 : !torch.vtensor<[65,65],bf16>, !torch.int, !torch.int -> !torch.vtensor<[65,65],bf16>
    %int68_197 = torch.constant.int 68
    %int65_198 = torch.constant.int 65
    %191 = torch.prim.ListConstruct %int68_197, %int65_198 : (!torch.int, !torch.int) -> !torch.list<int>
    %192 = torch.aten.view %189, %191 : !torch.vtensor<[4,17,65],bf16>, !torch.list<int> -> !torch.vtensor<[68,65],bf16>
    %193 = torch.aten.mm %192, %190 : !torch.vtensor<[68,65],bf16>, !torch.vtensor<[65,65],bf16> -> !torch.vtensor<[68,65],bf16>
    %int4_199 = torch.constant.int 4
    %int17_200 = torch.constant.int 17
    %int65_201 = torch.constant.int 65
    %194 = torch.prim.ListConstruct %int4_199, %int17_200, %int65_201 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %195 = torch.aten.view %193, %194 : !torch.vtensor<[68,65],bf16>, !torch.list<int> -> !torch.vtensor<[4,17,65],bf16>
    %int1_202 = torch.constant.int 1
    %196 = torch.aten.add.Tensor %195, %21, %int1_202 : !torch.vtensor<[4,17,65],bf16>, !torch.vtensor<[65],bf16>, !torch.int -> !torch.vtensor<[4,17,65],bf16>
    %float2.773500e-01_203 = torch.constant.float 0.27735009811261457
    %197 = torch.aten.mul.Scalar %196, %float2.773500e-01_203 : !torch.vtensor<[4,17,65],bf16>, !torch.float -> !torch.vtensor<[4,17,65],bf16>
    %int-2_204 = torch.constant.int -2
    %int-1_205 = torch.constant.int -1
    %198 = torch.aten.transpose.int %22, %int-2_204, %int-1_205 : !torch.vtensor<[65,65],bf16>, !torch.int, !torch.int -> !torch.vtensor<[65,65],bf16>
    %int68_206 = torch.constant.int 68
    %int65_207 = torch.constant.int 65
    %199 = torch.prim.ListConstruct %int68_206, %int65_207 : (!torch.int, !torch.int) -> !torch.list<int>
    %200 = torch.aten.view %189, %199 : !torch.vtensor<[4,17,65],bf16>, !torch.list<int> -> !torch.vtensor<[68,65],bf16>
    %201 = torch.aten.mm %200, %198 : !torch.vtensor<[68,65],bf16>, !torch.vtensor<[65,65],bf16> -> !torch.vtensor<[68,65],bf16>
    %int4_208 = torch.constant.int 4
    %int17_209 = torch.constant.int 17
    %int65_210 = torch.constant.int 65
    %202 = torch.prim.ListConstruct %int4_208, %int17_209, %int65_210 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %203 = torch.aten.view %201, %202 : !torch.vtensor<[68,65],bf16>, !torch.list<int> -> !torch.vtensor<[4,17,65],bf16>
    %int1_211 = torch.constant.int 1
    %204 = torch.aten.add.Tensor %203, %23, %int1_211 : !torch.vtensor<[4,17,65],bf16>, !torch.vtensor<[65],bf16>, !torch.int -> !torch.vtensor<[4,17,65],bf16>
    %int4_212 = torch.constant.int 4
    %int-1_213 = torch.constant.int -1
    %int5_214 = torch.constant.int 5
    %int13_215 = torch.constant.int 13
    %205 = torch.prim.ListConstruct %int4_212, %int-1_213, %int5_214, %int13_215 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %206 = torch.aten.view %204, %205 : !torch.vtensor<[4,17,65],bf16>, !torch.list<int> -> !torch.vtensor<[4,17,5,13],bf16>
    %int1_216 = torch.constant.int 1
    %int2_217 = torch.constant.int 2
    %207 = torch.aten.transpose.int %206, %int1_216, %int2_217 : !torch.vtensor<[4,17,5,13],bf16>, !torch.int, !torch.int -> !torch.vtensor<[4,5,17,13],bf16>
    %int0_218 = torch.constant.int 0
    %208 = torch.aten.clone %207, %int0_218 : !torch.vtensor<[4,5,17,13],bf16>, !torch.int -> !torch.vtensor<[4,5,17,13],bf16>
    %int-2_219 = torch.constant.int -2
    %int-1_220 = torch.constant.int -1
    %209 = torch.aten.transpose.int %24, %int-2_219, %int-1_220 : !torch.vtensor<[65,65],bf16>, !torch.int, !torch.int -> !torch.vtensor<[65,65],bf16>
    %int68_221 = torch.constant.int 68
    %int65_222 = torch.constant.int 65
    %210 = torch.prim.ListConstruct %int68_221, %int65_222 : (!torch.int, !torch.int) -> !torch.list<int>
    %211 = torch.aten.view %189, %210 : !torch.vtensor<[4,17,65],bf16>, !torch.list<int> -> !torch.vtensor<[68,65],bf16>
    %212 = torch.aten.mm %211, %209 : !torch.vtensor<[68,65],bf16>, !torch.vtensor<[65,65],bf16> -> !torch.vtensor<[68,65],bf16>
    %int4_223 = torch.constant.int 4
    %int17_224 = torch.constant.int 17
    %int65_225 = torch.constant.int 65
    %213 = torch.prim.ListConstruct %int4_223, %int17_224, %int65_225 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %214 = torch.aten.view %212, %213 : !torch.vtensor<[68,65],bf16>, !torch.list<int> -> !torch.vtensor<[4,17,65],bf16>
    %int1_226 = torch.constant.int 1
    %215 = torch.aten.add.Tensor %214, %25, %int1_226 : !torch.vtensor<[4,17,65],bf16>, !torch.vtensor<[65],bf16>, !torch.int -> !torch.vtensor<[4,17,65],bf16>
    %int4_227 = torch.constant.int 4
    %int-1_228 = torch.constant.int -1
    %int5_229 = torch.constant.int 5
    %int13_230 = torch.constant.int 13
    %216 = torch.prim.ListConstruct %int4_227, %int-1_228, %int5_229, %int13_230 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %217 = torch.aten.view %215, %216 : !torch.vtensor<[4,17,65],bf16>, !torch.list<int> -> !torch.vtensor<[4,17,5,13],bf16>
    %int1_231 = torch.constant.int 1
    %int2_232 = torch.constant.int 2
    %218 = torch.aten.transpose.int %217, %int1_231, %int2_232 : !torch.vtensor<[4,17,5,13],bf16>, !torch.int, !torch.int -> !torch.vtensor<[4,5,17,13],bf16>
    %int0_233 = torch.constant.int 0
    %219 = torch.aten.clone %218, %int0_233 : !torch.vtensor<[4,5,17,13],bf16>, !torch.int -> !torch.vtensor<[4,5,17,13],bf16>
    %int4_234 = torch.constant.int 4
    %int17_235 = torch.constant.int 17
    %int5_236 = torch.constant.int 5
    %int13_237 = torch.constant.int 13
    %220 = torch.prim.ListConstruct %int4_234, %int17_235, %int5_236, %int13_237 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %221 = torch.aten.view %197, %220 : !torch.vtensor<[4,17,65],bf16>, !torch.list<int> -> !torch.vtensor<[4,17,5,13],bf16>
    %int1_238 = torch.constant.int 1
    %int2_239 = torch.constant.int 2
    %222 = torch.aten.transpose.int %221, %int1_238, %int2_239 : !torch.vtensor<[4,17,5,13],bf16>, !torch.int, !torch.int -> !torch.vtensor<[4,5,17,13],bf16>
    %int0_240 = torch.constant.int 0
    %223 = torch.aten.clone %222, %int0_240 : !torch.vtensor<[4,5,17,13],bf16>, !torch.int -> !torch.vtensor<[4,5,17,13],bf16>
    %int20_241 = torch.constant.int 20
    %int-1_242 = torch.constant.int -1
    %int13_243 = torch.constant.int 13
    %224 = torch.prim.ListConstruct %int20_241, %int-1_242, %int13_243 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %225 = torch.aten.view %223, %224 : !torch.vtensor<[4,5,17,13],bf16>, !torch.list<int> -> !torch.vtensor<[20,17,13],bf16>
    %int20_244 = torch.constant.int 20
    %int-1_245 = torch.constant.int -1
    %int13_246 = torch.constant.int 13
    %226 = torch.prim.ListConstruct %int20_244, %int-1_245, %int13_246 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %227 = torch.aten.view %208, %226 : !torch.vtensor<[4,5,17,13],bf16>, !torch.list<int> -> !torch.vtensor<[20,17,13],bf16>
    %int20_247 = torch.constant.int 20
    %int-1_248 = torch.constant.int -1
    %int13_249 = torch.constant.int 13
    %228 = torch.prim.ListConstruct %int20_247, %int-1_248, %int13_249 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %229 = torch.aten.view %219, %228 : !torch.vtensor<[4,5,17,13],bf16>, !torch.list<int> -> !torch.vtensor<[20,17,13],bf16>
    %int1_250 = torch.constant.int 1
    %int2_251 = torch.constant.int 2
    %230 = torch.aten.transpose.int %227, %int1_250, %int2_251 : !torch.vtensor<[20,17,13],bf16>, !torch.int, !torch.int -> !torch.vtensor<[20,13,17],bf16>
    %int20_252 = torch.constant.int 20
    %int17_253 = torch.constant.int 17
    %int13_254 = torch.constant.int 13
    %231 = torch.prim.ListConstruct %int20_252, %int17_253, %int13_254 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_255 = torch.constant.bool false
    %232 = torch.aten.expand %225, %231, %false_255 : !torch.vtensor<[20,17,13],bf16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[20,17,13],bf16>
    %int20_256 = torch.constant.int 20
    %int17_257 = torch.constant.int 17
    %int13_258 = torch.constant.int 13
    %233 = torch.prim.ListConstruct %int20_256, %int17_257, %int13_258 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %234 = torch.aten.view %232, %233 : !torch.vtensor<[20,17,13],bf16>, !torch.list<int> -> !torch.vtensor<[20,17,13],bf16>
    %int20_259 = torch.constant.int 20
    %int13_260 = torch.constant.int 13
    %int17_261 = torch.constant.int 17
    %235 = torch.prim.ListConstruct %int20_259, %int13_260, %int17_261 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_262 = torch.constant.bool false
    %236 = torch.aten.expand %230, %235, %false_262 : !torch.vtensor<[20,13,17],bf16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[20,13,17],bf16>
    %int20_263 = torch.constant.int 20
    %int13_264 = torch.constant.int 13
    %int17_265 = torch.constant.int 17
    %237 = torch.prim.ListConstruct %int20_263, %int13_264, %int17_265 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %238 = torch.aten.view %236, %237 : !torch.vtensor<[20,13,17],bf16>, !torch.list<int> -> !torch.vtensor<[20,13,17],bf16>
    %239 = torch.aten.bmm %234, %238 : !torch.vtensor<[20,17,13],bf16>, !torch.vtensor<[20,13,17],bf16> -> !torch.vtensor<[20,17,17],bf16>
    %int20_266 = torch.constant.int 20
    %int17_267 = torch.constant.int 17
    %int17_268 = torch.constant.int 17
    %240 = torch.prim.ListConstruct %int20_266, %int17_267, %int17_268 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %241 = torch.aten.view %239, %240 : !torch.vtensor<[20,17,17],bf16>, !torch.list<int> -> !torch.vtensor<[20,17,17],bf16>
    %int4_269 = torch.constant.int 4
    %int5_270 = torch.constant.int 5
    %int17_271 = torch.constant.int 17
    %int17_272 = torch.constant.int 17
    %242 = torch.prim.ListConstruct %int4_269, %int5_270, %int17_271, %int17_272 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %243 = torch.aten.view %241, %242 : !torch.vtensor<[20,17,17],bf16>, !torch.list<int> -> !torch.vtensor<[4,5,17,17],bf16>
    %int1_273 = torch.constant.int 1
    %244 = torch.aten.add.Tensor %243, %61, %int1_273 : !torch.vtensor<[4,5,17,17],bf16>, !torch.vtensor<[4,1,17,17],bf16>, !torch.int -> !torch.vtensor<[4,5,17,17],bf16>
    %int20_274 = torch.constant.int 20
    %int17_275 = torch.constant.int 17
    %int17_276 = torch.constant.int 17
    %245 = torch.prim.ListConstruct %int20_274, %int17_275, %int17_276 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %246 = torch.aten.view %244, %245 : !torch.vtensor<[4,5,17,17],bf16>, !torch.list<int> -> !torch.vtensor<[20,17,17],bf16>
    %int-1_277 = torch.constant.int -1
    %false_278 = torch.constant.bool false
    %247 = torch.aten._softmax %246, %int-1_277, %false_278 : !torch.vtensor<[20,17,17],bf16>, !torch.int, !torch.bool -> !torch.vtensor<[20,17,17],bf16>
    %int20_279 = torch.constant.int 20
    %int17_280 = torch.constant.int 17
    %int17_281 = torch.constant.int 17
    %248 = torch.prim.ListConstruct %int20_279, %int17_280, %int17_281 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_282 = torch.constant.bool false
    %249 = torch.aten.expand %247, %248, %false_282 : !torch.vtensor<[20,17,17],bf16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[20,17,17],bf16>
    %int20_283 = torch.constant.int 20
    %int17_284 = torch.constant.int 17
    %int17_285 = torch.constant.int 17
    %250 = torch.prim.ListConstruct %int20_283, %int17_284, %int17_285 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %251 = torch.aten.view %249, %250 : !torch.vtensor<[20,17,17],bf16>, !torch.list<int> -> !torch.vtensor<[20,17,17],bf16>
    %int20_286 = torch.constant.int 20
    %int17_287 = torch.constant.int 17
    %int13_288 = torch.constant.int 13
    %252 = torch.prim.ListConstruct %int20_286, %int17_287, %int13_288 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_289 = torch.constant.bool false
    %253 = torch.aten.expand %229, %252, %false_289 : !torch.vtensor<[20,17,13],bf16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[20,17,13],bf16>
    %int20_290 = torch.constant.int 20
    %int17_291 = torch.constant.int 17
    %int13_292 = torch.constant.int 13
    %254 = torch.prim.ListConstruct %int20_290, %int17_291, %int13_292 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %255 = torch.aten.view %253, %254 : !torch.vtensor<[20,17,13],bf16>, !torch.list<int> -> !torch.vtensor<[20,17,13],bf16>
    %256 = torch.aten.bmm %251, %255 : !torch.vtensor<[20,17,17],bf16>, !torch.vtensor<[20,17,13],bf16> -> !torch.vtensor<[20,17,13],bf16>
    %int20_293 = torch.constant.int 20
    %int17_294 = torch.constant.int 17
    %int13_295 = torch.constant.int 13
    %257 = torch.prim.ListConstruct %int20_293, %int17_294, %int13_295 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %258 = torch.aten.view %256, %257 : !torch.vtensor<[20,17,13],bf16>, !torch.list<int> -> !torch.vtensor<[20,17,13],bf16>
    %int4_296 = torch.constant.int 4
    %int5_297 = torch.constant.int 5
    %int17_298 = torch.constant.int 17
    %int13_299 = torch.constant.int 13
    %259 = torch.prim.ListConstruct %int4_296, %int5_297, %int17_298, %int13_299 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %260 = torch.aten.view %258, %259 : !torch.vtensor<[20,17,13],bf16>, !torch.list<int> -> !torch.vtensor<[4,5,17,13],bf16>
    %int1_300 = torch.constant.int 1
    %int2_301 = torch.constant.int 2
    %261 = torch.aten.transpose.int %260, %int1_300, %int2_301 : !torch.vtensor<[4,5,17,13],bf16>, !torch.int, !torch.int -> !torch.vtensor<[4,17,5,13],bf16>
    %int0_302 = torch.constant.int 0
    %262 = torch.aten.clone %261, %int0_302 : !torch.vtensor<[4,17,5,13],bf16>, !torch.int -> !torch.vtensor<[4,17,5,13],bf16>
    %int4_303 = torch.constant.int 4
    %int17_304 = torch.constant.int 17
    %int65_305 = torch.constant.int 65
    %263 = torch.prim.ListConstruct %int4_303, %int17_304, %int65_305 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %264 = torch.aten._unsafe_view %262, %263 : !torch.vtensor<[4,17,5,13],bf16>, !torch.list<int> -> !torch.vtensor<[4,17,65],bf16>
    %int-2_306 = torch.constant.int -2
    %int-1_307 = torch.constant.int -1
    %265 = torch.aten.transpose.int %26, %int-2_306, %int-1_307 : !torch.vtensor<[65,65],bf16>, !torch.int, !torch.int -> !torch.vtensor<[65,65],bf16>
    %int68_308 = torch.constant.int 68
    %int65_309 = torch.constant.int 65
    %266 = torch.prim.ListConstruct %int68_308, %int65_309 : (!torch.int, !torch.int) -> !torch.list<int>
    %267 = torch.aten.view %264, %266 : !torch.vtensor<[4,17,65],bf16>, !torch.list<int> -> !torch.vtensor<[68,65],bf16>
    %268 = torch.aten.mm %267, %265 : !torch.vtensor<[68,65],bf16>, !torch.vtensor<[65,65],bf16> -> !torch.vtensor<[68,65],bf16>
    %int4_310 = torch.constant.int 4
    %int17_311 = torch.constant.int 17
    %int65_312 = torch.constant.int 65
    %269 = torch.prim.ListConstruct %int4_310, %int17_311, %int65_312 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %270 = torch.aten.view %268, %269 : !torch.vtensor<[68,65],bf16>, !torch.list<int> -> !torch.vtensor<[4,17,65],bf16>
    %int1_313 = torch.constant.int 1
    %271 = torch.aten.add.Tensor %270, %27, %int1_313 : !torch.vtensor<[4,17,65],bf16>, !torch.vtensor<[65],bf16>, !torch.int -> !torch.vtensor<[4,17,65],bf16>
    %int1_314 = torch.constant.int 1
    %272 = torch.aten.add.Tensor %180, %271, %int1_314 : !torch.vtensor<[4,17,65],bf16>, !torch.vtensor<[4,17,65],bf16>, !torch.int -> !torch.vtensor<[4,17,65],bf16>
    %int6_315 = torch.constant.int 6
    %273 = torch.prims.convert_element_type %272, %int6_315 : !torch.vtensor<[4,17,65],bf16>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %int2_316 = torch.constant.int 2
    %274 = torch.prim.ListConstruct %int2_316 : (!torch.int) -> !torch.list<int>
    %int0_317 = torch.constant.int 0
    %true_318 = torch.constant.bool true
    %result0_319, %result1_320 = torch.aten.var_mean.correction %273, %274, %int0_317, %true_318 : !torch.vtensor<[4,17,65],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[4,17,1],f32>, !torch.vtensor<[4,17,1],f32>
    %float1.000000e-04_321 = torch.constant.float 1.000000e-04
    %int1_322 = torch.constant.int 1
    %275 = torch.aten.add.Scalar %result0_319, %float1.000000e-04_321, %int1_322 : !torch.vtensor<[4,17,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,17,1],f32>
    %276 = torch.aten.rsqrt %275 : !torch.vtensor<[4,17,1],f32> -> !torch.vtensor<[4,17,1],f32>
    %int1_323 = torch.constant.int 1
    %277 = torch.aten.sub.Tensor %272, %result1_320, %int1_323 : !torch.vtensor<[4,17,65],bf16>, !torch.vtensor<[4,17,1],f32>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %278 = torch.aten.mul.Tensor %277, %276 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[4,17,1],f32> -> !torch.vtensor<[4,17,65],f32>
    %279 = torch.aten.mul.Tensor %278, %28 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[65],bf16> -> !torch.vtensor<[4,17,65],f32>
    %int1_324 = torch.constant.int 1
    %280 = torch.aten.add.Tensor %279, %29, %int1_324 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[65],bf16>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %int15_325 = torch.constant.int 15
    %281 = torch.prims.convert_element_type %280, %int15_325 : !torch.vtensor<[4,17,65],f32>, !torch.int -> !torch.vtensor<[4,17,65],bf16>
    %int-2_326 = torch.constant.int -2
    %int-1_327 = torch.constant.int -1
    %282 = torch.aten.transpose.int %30, %int-2_326, %int-1_327 : !torch.vtensor<[7,65],bf16>, !torch.int, !torch.int -> !torch.vtensor<[65,7],bf16>
    %int68_328 = torch.constant.int 68
    %int65_329 = torch.constant.int 65
    %283 = torch.prim.ListConstruct %int68_328, %int65_329 : (!torch.int, !torch.int) -> !torch.list<int>
    %284 = torch.aten.view %281, %283 : !torch.vtensor<[4,17,65],bf16>, !torch.list<int> -> !torch.vtensor<[68,65],bf16>
    %285 = torch.aten.mm %284, %282 : !torch.vtensor<[68,65],bf16>, !torch.vtensor<[65,7],bf16> -> !torch.vtensor<[68,7],bf16>
    %int4_330 = torch.constant.int 4
    %int17_331 = torch.constant.int 17
    %int7_332 = torch.constant.int 7
    %286 = torch.prim.ListConstruct %int4_330, %int17_331, %int7_332 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %287 = torch.aten.view %285, %286 : !torch.vtensor<[68,7],bf16>, !torch.list<int> -> !torch.vtensor<[4,17,7],bf16>
    %int1_333 = torch.constant.int 1
    %288 = torch.aten.add.Tensor %287, %31, %int1_333 : !torch.vtensor<[4,17,7],bf16>, !torch.vtensor<[7],bf16>, !torch.int -> !torch.vtensor<[4,17,7],bf16>
    %float1.702000e00_334 = torch.constant.float 1.702000e+00
    %289 = torch.aten.mul.Scalar %288, %float1.702000e00_334 : !torch.vtensor<[4,17,7],bf16>, !torch.float -> !torch.vtensor<[4,17,7],bf16>
    %290 = torch.aten.sigmoid %289 : !torch.vtensor<[4,17,7],bf16> -> !torch.vtensor<[4,17,7],bf16>
    %291 = torch.aten.mul.Tensor %288, %290 : !torch.vtensor<[4,17,7],bf16>, !torch.vtensor<[4,17,7],bf16> -> !torch.vtensor<[4,17,7],bf16>
    %int-2_335 = torch.constant.int -2
    %int-1_336 = torch.constant.int -1
    %292 = torch.aten.transpose.int %32, %int-2_335, %int-1_336 : !torch.vtensor<[65,7],bf16>, !torch.int, !torch.int -> !torch.vtensor<[7,65],bf16>
    %int68_337 = torch.constant.int 68
    %int7_338 = torch.constant.int 7
    %293 = torch.prim.ListConstruct %int68_337, %int7_338 : (!torch.int, !torch.int) -> !torch.list<int>
    %294 = torch.aten.view %291, %293 : !torch.vtensor<[4,17,7],bf16>, !torch.list<int> -> !torch.vtensor<[68,7],bf16>
    %295 = torch.aten.mm %294, %292 : !torch.vtensor<[68,7],bf16>, !torch.vtensor<[7,65],bf16> -> !torch.vtensor<[68,65],bf16>
    %int4_339 = torch.constant.int 4
    %int17_340 = torch.constant.int 17
    %int65_341 = torch.constant.int 65
    %296 = torch.prim.ListConstruct %int4_339, %int17_340, %int65_341 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %297 = torch.aten.view %295, %296 : !torch.vtensor<[68,65],bf16>, !torch.list<int> -> !torch.vtensor<[4,17,65],bf16>
    %int1_342 = torch.constant.int 1
    %298 = torch.aten.add.Tensor %297, %33, %int1_342 : !torch.vtensor<[4,17,65],bf16>, !torch.vtensor<[65],bf16>, !torch.int -> !torch.vtensor<[4,17,65],bf16>
    %int1_343 = torch.constant.int 1
    %299 = torch.aten.add.Tensor %272, %298, %int1_343 : !torch.vtensor<[4,17,65],bf16>, !torch.vtensor<[4,17,65],bf16>, !torch.int -> !torch.vtensor<[4,17,65],bf16>
    %int6_344 = torch.constant.int 6
    %300 = torch.prims.convert_element_type %299, %int6_344 : !torch.vtensor<[4,17,65],bf16>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %int2_345 = torch.constant.int 2
    %301 = torch.prim.ListConstruct %int2_345 : (!torch.int) -> !torch.list<int>
    %int0_346 = torch.constant.int 0
    %true_347 = torch.constant.bool true
    %result0_348, %result1_349 = torch.aten.var_mean.correction %300, %301, %int0_346, %true_347 : !torch.vtensor<[4,17,65],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[4,17,1],f32>, !torch.vtensor<[4,17,1],f32>
    %float1.000000e-04_350 = torch.constant.float 1.000000e-04
    %int1_351 = torch.constant.int 1
    %302 = torch.aten.add.Scalar %result0_348, %float1.000000e-04_350, %int1_351 : !torch.vtensor<[4,17,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,17,1],f32>
    %303 = torch.aten.rsqrt %302 : !torch.vtensor<[4,17,1],f32> -> !torch.vtensor<[4,17,1],f32>
    %int1_352 = torch.constant.int 1
    %304 = torch.aten.sub.Tensor %299, %result1_349, %int1_352 : !torch.vtensor<[4,17,65],bf16>, !torch.vtensor<[4,17,1],f32>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %305 = torch.aten.mul.Tensor %304, %303 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[4,17,1],f32> -> !torch.vtensor<[4,17,65],f32>
    %306 = torch.aten.mul.Tensor %305, %34 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[65],bf16> -> !torch.vtensor<[4,17,65],f32>
    %int1_353 = torch.constant.int 1
    %307 = torch.aten.add.Tensor %306, %35, %int1_353 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[65],bf16>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %int15_354 = torch.constant.int 15
    %308 = torch.prims.convert_element_type %307, %int15_354 : !torch.vtensor<[4,17,65],f32>, !torch.int -> !torch.vtensor<[4,17,65],bf16>
    %int4_355 = torch.constant.int 4
    %none_356 = torch.constant.none
    %none_357 = torch.constant.none
    %cpu_358 = torch.constant.device "cpu"
    %false_359 = torch.constant.bool false
    %309 = torch.aten.arange %int4_355, %none_356, %none_357, %cpu_358, %false_359 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[4],si64>
    %int10 = torch.constant.int 10
    %310 = torch.aten.eq.Scalar %38, %int10 : !torch.vtensor<[4,17],si64>, !torch.int -> !torch.vtensor<[4,17],i1>
    %int3_360 = torch.constant.int 3
    %311 = torch.prims.convert_element_type %310, %int3_360 : !torch.vtensor<[4,17],i1>, !torch.int -> !torch.vtensor<[4,17],si32>
    %int-1_361 = torch.constant.int -1
    %false_362 = torch.constant.bool false
    %312 = torch.aten.argmax %311, %int-1_361, %false_362 : !torch.vtensor<[4,17],si32>, !torch.int, !torch.bool -> !torch.vtensor<[4],si64>
    %313 = torch.prim.ListConstruct %309, %312 : (!torch.vtensor<[4],si64>, !torch.vtensor<[4],si64>) -> !torch.list<optional<vtensor>>
    %314 = torch.aten.index.Tensor %308, %313 : !torch.vtensor<[4,17,65],bf16>, !torch.list<optional<vtensor>> -> !torch.vtensor<[4,65],bf16>
    return %308, %314 : !torch.vtensor<[4,17,65],bf16>, !torch.vtensor<[4,65],bf16>
  }
}

{-#
  dialect_resources: {
    builtin: {
      torch_tensor_1_17_torch.int64: "0x0800000000000000000000000100000000000000020000000000000003000000000000000400000000000000050000000000000006000000000000000700000000000000080000000000000009000000000000000A000000000000000B000000000000000C000000000000000D000000000000000E000000000000000F000000000000001000000000000000"
    }
  }
#-}
