module @module {
  util.global private @__auto.text_model.embeddings.token_embedding.weight = #stream.parameter.named<"model"::"text_model.embeddings.token_embedding.weight"> : tensor<11x65xf32>
  util.global private @__auto.text_model.embeddings.position_embedding.weight = #stream.parameter.named<"model"::"text_model.embeddings.position_embedding.weight"> : tensor<17x65xf32>
  util.global private @__auto.text_model.encoder.layers.0.layer_norm1.weight = #stream.parameter.named<"model"::"text_model.encoder.layers.0.layer_norm1.weight"> : tensor<65xf32>
  util.global private @__auto.text_model.encoder.layers.0.layer_norm1.bias = #stream.parameter.named<"model"::"text_model.encoder.layers.0.layer_norm1.bias"> : tensor<65xf32>
  util.global private @__auto.text_model.encoder.layers.0.self_attn.q_proj.weight = #stream.parameter.named<"model"::"text_model.encoder.layers.0.self_attn.q_proj.weight"> : tensor<65x65xf32>
  util.global private @__auto.text_model.encoder.layers.0.self_attn.q_proj.bias = #stream.parameter.named<"model"::"text_model.encoder.layers.0.self_attn.q_proj.bias"> : tensor<65xf32>
  util.global private @__auto.text_model.encoder.layers.0.self_attn.k_proj.weight = #stream.parameter.named<"model"::"text_model.encoder.layers.0.self_attn.k_proj.weight"> : tensor<65x65xf32>
  util.global private @__auto.text_model.encoder.layers.0.self_attn.k_proj.bias = #stream.parameter.named<"model"::"text_model.encoder.layers.0.self_attn.k_proj.bias"> : tensor<65xf32>
  util.global private @__auto.text_model.encoder.layers.0.self_attn.v_proj.weight = #stream.parameter.named<"model"::"text_model.encoder.layers.0.self_attn.v_proj.weight"> : tensor<65x65xf32>
  util.global private @__auto.text_model.encoder.layers.0.self_attn.v_proj.bias = #stream.parameter.named<"model"::"text_model.encoder.layers.0.self_attn.v_proj.bias"> : tensor<65xf32>
  util.global private @__auto.text_model.encoder.layers.0.self_attn.out_proj.weight = #stream.parameter.named<"model"::"text_model.encoder.layers.0.self_attn.out_proj.weight"> : tensor<65x65xf32>
  util.global private @__auto.text_model.encoder.layers.0.self_attn.out_proj.bias = #stream.parameter.named<"model"::"text_model.encoder.layers.0.self_attn.out_proj.bias"> : tensor<65xf32>
  util.global private @__auto.text_model.encoder.layers.0.layer_norm2.weight = #stream.parameter.named<"model"::"text_model.encoder.layers.0.layer_norm2.weight"> : tensor<65xf32>
  util.global private @__auto.text_model.encoder.layers.0.layer_norm2.bias = #stream.parameter.named<"model"::"text_model.encoder.layers.0.layer_norm2.bias"> : tensor<65xf32>
  util.global private @__auto.text_model.encoder.layers.0.mlp.fc1.weight = #stream.parameter.named<"model"::"text_model.encoder.layers.0.mlp.fc1.weight"> : tensor<7x65xf32>
  util.global private @__auto.text_model.encoder.layers.0.mlp.fc1.bias = #stream.parameter.named<"model"::"text_model.encoder.layers.0.mlp.fc1.bias"> : tensor<7xf32>
  util.global private @__auto.text_model.encoder.layers.0.mlp.fc2.weight = #stream.parameter.named<"model"::"text_model.encoder.layers.0.mlp.fc2.weight"> : tensor<65x7xf32>
  util.global private @__auto.text_model.encoder.layers.0.mlp.fc2.bias = #stream.parameter.named<"model"::"text_model.encoder.layers.0.mlp.fc2.bias"> : tensor<65xf32>
  util.global private @__auto.text_model.encoder.layers.1.layer_norm1.weight = #stream.parameter.named<"model"::"text_model.encoder.layers.1.layer_norm1.weight"> : tensor<65xf32>
  util.global private @__auto.text_model.encoder.layers.1.layer_norm1.bias = #stream.parameter.named<"model"::"text_model.encoder.layers.1.layer_norm1.bias"> : tensor<65xf32>
  util.global private @__auto.text_model.encoder.layers.1.self_attn.q_proj.weight = #stream.parameter.named<"model"::"text_model.encoder.layers.1.self_attn.q_proj.weight"> : tensor<65x65xf32>
  util.global private @__auto.text_model.encoder.layers.1.self_attn.q_proj.bias = #stream.parameter.named<"model"::"text_model.encoder.layers.1.self_attn.q_proj.bias"> : tensor<65xf32>
  util.global private @__auto.text_model.encoder.layers.1.self_attn.k_proj.weight = #stream.parameter.named<"model"::"text_model.encoder.layers.1.self_attn.k_proj.weight"> : tensor<65x65xf32>
  util.global private @__auto.text_model.encoder.layers.1.self_attn.k_proj.bias = #stream.parameter.named<"model"::"text_model.encoder.layers.1.self_attn.k_proj.bias"> : tensor<65xf32>
  util.global private @__auto.text_model.encoder.layers.1.self_attn.v_proj.weight = #stream.parameter.named<"model"::"text_model.encoder.layers.1.self_attn.v_proj.weight"> : tensor<65x65xf32>
  util.global private @__auto.text_model.encoder.layers.1.self_attn.v_proj.bias = #stream.parameter.named<"model"::"text_model.encoder.layers.1.self_attn.v_proj.bias"> : tensor<65xf32>
  util.global private @__auto.text_model.encoder.layers.1.self_attn.out_proj.weight = #stream.parameter.named<"model"::"text_model.encoder.layers.1.self_attn.out_proj.weight"> : tensor<65x65xf32>
  util.global private @__auto.text_model.encoder.layers.1.self_attn.out_proj.bias = #stream.parameter.named<"model"::"text_model.encoder.layers.1.self_attn.out_proj.bias"> : tensor<65xf32>
  util.global private @__auto.text_model.encoder.layers.1.layer_norm2.weight = #stream.parameter.named<"model"::"text_model.encoder.layers.1.layer_norm2.weight"> : tensor<65xf32>
  util.global private @__auto.text_model.encoder.layers.1.layer_norm2.bias = #stream.parameter.named<"model"::"text_model.encoder.layers.1.layer_norm2.bias"> : tensor<65xf32>
  util.global private @__auto.text_model.encoder.layers.1.mlp.fc1.weight = #stream.parameter.named<"model"::"text_model.encoder.layers.1.mlp.fc1.weight"> : tensor<7x65xf32>
  util.global private @__auto.text_model.encoder.layers.1.mlp.fc1.bias = #stream.parameter.named<"model"::"text_model.encoder.layers.1.mlp.fc1.bias"> : tensor<7xf32>
  util.global private @__auto.text_model.encoder.layers.1.mlp.fc2.weight = #stream.parameter.named<"model"::"text_model.encoder.layers.1.mlp.fc2.weight"> : tensor<65x7xf32>
  util.global private @__auto.text_model.encoder.layers.1.mlp.fc2.bias = #stream.parameter.named<"model"::"text_model.encoder.layers.1.mlp.fc2.bias"> : tensor<65xf32>
  util.global private @__auto.text_model.final_layer_norm.weight = #stream.parameter.named<"model"::"text_model.final_layer_norm.weight"> : tensor<65xf32>
  util.global private @__auto.text_model.final_layer_norm.bias = #stream.parameter.named<"model"::"text_model.final_layer_norm.bias"> : tensor<65xf32>
  func.func @forward_bs4(%arg0: !torch.vtensor<[4,17],si64>) -> (!torch.vtensor<[4,17,65],f32>, !torch.vtensor<[4,65],f32>) attributes {torch.assume_strict_symbolic_shapes} {
    %__auto.text_model.embeddings.token_embedding.weight = util.global.load @__auto.text_model.embeddings.token_embedding.weight : tensor<11x65xf32>
    %0 = torch_c.from_builtin_tensor %__auto.text_model.embeddings.token_embedding.weight : tensor<11x65xf32> -> !torch.vtensor<[11,65],f32>
    %__auto.text_model.embeddings.position_embedding.weight = util.global.load @__auto.text_model.embeddings.position_embedding.weight : tensor<17x65xf32>
    %1 = torch_c.from_builtin_tensor %__auto.text_model.embeddings.position_embedding.weight : tensor<17x65xf32> -> !torch.vtensor<[17,65],f32>
    %__auto.text_model.encoder.layers.0.layer_norm1.weight = util.global.load @__auto.text_model.encoder.layers.0.layer_norm1.weight : tensor<65xf32>
    %2 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.0.layer_norm1.weight : tensor<65xf32> -> !torch.vtensor<[65],f32>
    %__auto.text_model.encoder.layers.0.layer_norm1.bias = util.global.load @__auto.text_model.encoder.layers.0.layer_norm1.bias : tensor<65xf32>
    %3 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.0.layer_norm1.bias : tensor<65xf32> -> !torch.vtensor<[65],f32>
    %__auto.text_model.encoder.layers.0.self_attn.q_proj.weight = util.global.load @__auto.text_model.encoder.layers.0.self_attn.q_proj.weight : tensor<65x65xf32>
    %4 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.0.self_attn.q_proj.weight : tensor<65x65xf32> -> !torch.vtensor<[65,65],f32>
    %__auto.text_model.encoder.layers.0.self_attn.q_proj.bias = util.global.load @__auto.text_model.encoder.layers.0.self_attn.q_proj.bias : tensor<65xf32>
    %5 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.0.self_attn.q_proj.bias : tensor<65xf32> -> !torch.vtensor<[65],f32>
    %__auto.text_model.encoder.layers.0.self_attn.k_proj.weight = util.global.load @__auto.text_model.encoder.layers.0.self_attn.k_proj.weight : tensor<65x65xf32>
    %6 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.0.self_attn.k_proj.weight : tensor<65x65xf32> -> !torch.vtensor<[65,65],f32>
    %__auto.text_model.encoder.layers.0.self_attn.k_proj.bias = util.global.load @__auto.text_model.encoder.layers.0.self_attn.k_proj.bias : tensor<65xf32>
    %7 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.0.self_attn.k_proj.bias : tensor<65xf32> -> !torch.vtensor<[65],f32>
    %__auto.text_model.encoder.layers.0.self_attn.v_proj.weight = util.global.load @__auto.text_model.encoder.layers.0.self_attn.v_proj.weight : tensor<65x65xf32>
    %8 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.0.self_attn.v_proj.weight : tensor<65x65xf32> -> !torch.vtensor<[65,65],f32>
    %__auto.text_model.encoder.layers.0.self_attn.v_proj.bias = util.global.load @__auto.text_model.encoder.layers.0.self_attn.v_proj.bias : tensor<65xf32>
    %9 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.0.self_attn.v_proj.bias : tensor<65xf32> -> !torch.vtensor<[65],f32>
    %__auto.text_model.encoder.layers.0.self_attn.out_proj.weight = util.global.load @__auto.text_model.encoder.layers.0.self_attn.out_proj.weight : tensor<65x65xf32>
    %10 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.0.self_attn.out_proj.weight : tensor<65x65xf32> -> !torch.vtensor<[65,65],f32>
    %__auto.text_model.encoder.layers.0.self_attn.out_proj.bias = util.global.load @__auto.text_model.encoder.layers.0.self_attn.out_proj.bias : tensor<65xf32>
    %11 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.0.self_attn.out_proj.bias : tensor<65xf32> -> !torch.vtensor<[65],f32>
    %__auto.text_model.encoder.layers.0.layer_norm2.weight = util.global.load @__auto.text_model.encoder.layers.0.layer_norm2.weight : tensor<65xf32>
    %12 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.0.layer_norm2.weight : tensor<65xf32> -> !torch.vtensor<[65],f32>
    %__auto.text_model.encoder.layers.0.layer_norm2.bias = util.global.load @__auto.text_model.encoder.layers.0.layer_norm2.bias : tensor<65xf32>
    %13 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.0.layer_norm2.bias : tensor<65xf32> -> !torch.vtensor<[65],f32>
    %__auto.text_model.encoder.layers.0.mlp.fc1.weight = util.global.load @__auto.text_model.encoder.layers.0.mlp.fc1.weight : tensor<7x65xf32>
    %14 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.0.mlp.fc1.weight : tensor<7x65xf32> -> !torch.vtensor<[7,65],f32>
    %__auto.text_model.encoder.layers.0.mlp.fc1.bias = util.global.load @__auto.text_model.encoder.layers.0.mlp.fc1.bias : tensor<7xf32>
    %15 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.0.mlp.fc1.bias : tensor<7xf32> -> !torch.vtensor<[7],f32>
    %__auto.text_model.encoder.layers.0.mlp.fc2.weight = util.global.load @__auto.text_model.encoder.layers.0.mlp.fc2.weight : tensor<65x7xf32>
    %16 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.0.mlp.fc2.weight : tensor<65x7xf32> -> !torch.vtensor<[65,7],f32>
    %__auto.text_model.encoder.layers.0.mlp.fc2.bias = util.global.load @__auto.text_model.encoder.layers.0.mlp.fc2.bias : tensor<65xf32>
    %17 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.0.mlp.fc2.bias : tensor<65xf32> -> !torch.vtensor<[65],f32>
    %__auto.text_model.encoder.layers.1.layer_norm1.weight = util.global.load @__auto.text_model.encoder.layers.1.layer_norm1.weight : tensor<65xf32>
    %18 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.1.layer_norm1.weight : tensor<65xf32> -> !torch.vtensor<[65],f32>
    %__auto.text_model.encoder.layers.1.layer_norm1.bias = util.global.load @__auto.text_model.encoder.layers.1.layer_norm1.bias : tensor<65xf32>
    %19 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.1.layer_norm1.bias : tensor<65xf32> -> !torch.vtensor<[65],f32>
    %__auto.text_model.encoder.layers.1.self_attn.q_proj.weight = util.global.load @__auto.text_model.encoder.layers.1.self_attn.q_proj.weight : tensor<65x65xf32>
    %20 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.1.self_attn.q_proj.weight : tensor<65x65xf32> -> !torch.vtensor<[65,65],f32>
    %__auto.text_model.encoder.layers.1.self_attn.q_proj.bias = util.global.load @__auto.text_model.encoder.layers.1.self_attn.q_proj.bias : tensor<65xf32>
    %21 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.1.self_attn.q_proj.bias : tensor<65xf32> -> !torch.vtensor<[65],f32>
    %__auto.text_model.encoder.layers.1.self_attn.k_proj.weight = util.global.load @__auto.text_model.encoder.layers.1.self_attn.k_proj.weight : tensor<65x65xf32>
    %22 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.1.self_attn.k_proj.weight : tensor<65x65xf32> -> !torch.vtensor<[65,65],f32>
    %__auto.text_model.encoder.layers.1.self_attn.k_proj.bias = util.global.load @__auto.text_model.encoder.layers.1.self_attn.k_proj.bias : tensor<65xf32>
    %23 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.1.self_attn.k_proj.bias : tensor<65xf32> -> !torch.vtensor<[65],f32>
    %__auto.text_model.encoder.layers.1.self_attn.v_proj.weight = util.global.load @__auto.text_model.encoder.layers.1.self_attn.v_proj.weight : tensor<65x65xf32>
    %24 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.1.self_attn.v_proj.weight : tensor<65x65xf32> -> !torch.vtensor<[65,65],f32>
    %__auto.text_model.encoder.layers.1.self_attn.v_proj.bias = util.global.load @__auto.text_model.encoder.layers.1.self_attn.v_proj.bias : tensor<65xf32>
    %25 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.1.self_attn.v_proj.bias : tensor<65xf32> -> !torch.vtensor<[65],f32>
    %__auto.text_model.encoder.layers.1.self_attn.out_proj.weight = util.global.load @__auto.text_model.encoder.layers.1.self_attn.out_proj.weight : tensor<65x65xf32>
    %26 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.1.self_attn.out_proj.weight : tensor<65x65xf32> -> !torch.vtensor<[65,65],f32>
    %__auto.text_model.encoder.layers.1.self_attn.out_proj.bias = util.global.load @__auto.text_model.encoder.layers.1.self_attn.out_proj.bias : tensor<65xf32>
    %27 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.1.self_attn.out_proj.bias : tensor<65xf32> -> !torch.vtensor<[65],f32>
    %__auto.text_model.encoder.layers.1.layer_norm2.weight = util.global.load @__auto.text_model.encoder.layers.1.layer_norm2.weight : tensor<65xf32>
    %28 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.1.layer_norm2.weight : tensor<65xf32> -> !torch.vtensor<[65],f32>
    %__auto.text_model.encoder.layers.1.layer_norm2.bias = util.global.load @__auto.text_model.encoder.layers.1.layer_norm2.bias : tensor<65xf32>
    %29 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.1.layer_norm2.bias : tensor<65xf32> -> !torch.vtensor<[65],f32>
    %__auto.text_model.encoder.layers.1.mlp.fc1.weight = util.global.load @__auto.text_model.encoder.layers.1.mlp.fc1.weight : tensor<7x65xf32>
    %30 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.1.mlp.fc1.weight : tensor<7x65xf32> -> !torch.vtensor<[7,65],f32>
    %__auto.text_model.encoder.layers.1.mlp.fc1.bias = util.global.load @__auto.text_model.encoder.layers.1.mlp.fc1.bias : tensor<7xf32>
    %31 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.1.mlp.fc1.bias : tensor<7xf32> -> !torch.vtensor<[7],f32>
    %__auto.text_model.encoder.layers.1.mlp.fc2.weight = util.global.load @__auto.text_model.encoder.layers.1.mlp.fc2.weight : tensor<65x7xf32>
    %32 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.1.mlp.fc2.weight : tensor<65x7xf32> -> !torch.vtensor<[65,7],f32>
    %__auto.text_model.encoder.layers.1.mlp.fc2.bias = util.global.load @__auto.text_model.encoder.layers.1.mlp.fc2.bias : tensor<65xf32>
    %33 = torch_c.from_builtin_tensor %__auto.text_model.encoder.layers.1.mlp.fc2.bias : tensor<65xf32> -> !torch.vtensor<[65],f32>
    %__auto.text_model.final_layer_norm.weight = util.global.load @__auto.text_model.final_layer_norm.weight : tensor<65xf32>
    %34 = torch_c.from_builtin_tensor %__auto.text_model.final_layer_norm.weight : tensor<65xf32> -> !torch.vtensor<[65],f32>
    %__auto.text_model.final_layer_norm.bias = util.global.load @__auto.text_model.final_layer_norm.bias : tensor<65xf32>
    %35 = torch_c.from_builtin_tensor %__auto.text_model.final_layer_norm.bias : tensor<65xf32> -> !torch.vtensor<[65],f32>
    %36 = torch.symbolic_int "s0" {min_val = 2, max_val = 9223372036854775806} : !torch.int
    %int-1 = torch.constant.int -1
    %int17 = torch.constant.int 17
    %37 = torch.prim.ListConstruct %int-1, %int17 : (!torch.int, !torch.int) -> !torch.list<int>
    %38 = torch.aten.view %arg0, %37 : !torch.vtensor<[4,17],si64>, !torch.list<int> -> !torch.vtensor<[4,17],si64>
    %39 = torch.vtensor.literal(dense_resource<torch_tensor_1_17_torch.int64> : tensor<1x17xsi64>) : !torch.vtensor<[1,17],si64>
    %int0 = torch.constant.int 0
    %int0_0 = torch.constant.int 0
    %int9223372036854775807 = torch.constant.int 9223372036854775807
    %int1 = torch.constant.int 1
    %40 = torch.aten.slice.Tensor %39, %int0, %int0_0, %int9223372036854775807, %int1 : !torch.vtensor<[1,17],si64>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,17],si64>
    %int-1_1 = torch.constant.int -1
    %false = torch.constant.bool false
    %false_2 = torch.constant.bool false
    %41 = torch.aten.embedding %0, %38, %int-1_1, %false, %false_2 : !torch.vtensor<[11,65],f32>, !torch.vtensor<[4,17],si64>, !torch.int, !torch.bool, !torch.bool -> !torch.vtensor<[4,17,65],f32>
    %int-1_3 = torch.constant.int -1
    %false_4 = torch.constant.bool false
    %false_5 = torch.constant.bool false
    %42 = torch.aten.embedding %1, %40, %int-1_3, %false_4, %false_5 : !torch.vtensor<[17,65],f32>, !torch.vtensor<[1,17],si64>, !torch.int, !torch.bool, !torch.bool -> !torch.vtensor<[1,17,65],f32>
    %int1_6 = torch.constant.int 1
    %43 = torch.aten.add.Tensor %41, %42, %int1_6 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[1,17,65],f32>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %int17_7 = torch.constant.int 17
    %int17_8 = torch.constant.int 17
    %44 = torch.prim.ListConstruct %int17_7, %int17_8 : (!torch.int, !torch.int) -> !torch.list<int>
    %int17_9 = torch.constant.int 17
    %int1_10 = torch.constant.int 1
    %45 = torch.prim.ListConstruct %int17_9, %int1_10 : (!torch.int, !torch.int) -> !torch.list<int>
    %int6 = torch.constant.int 6
    %int0_11 = torch.constant.int 0
    %cpu = torch.constant.device "cpu"
    %false_12 = torch.constant.bool false
    %46 = torch.aten.empty_strided %44, %45, %int6, %int0_11, %cpu, %false_12 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[17,17],f32>
    %float-3.402820e38 = torch.constant.float -3.4028234663852886E+38
    %47 = torch.aten.fill.Scalar %46, %float-3.402820e38 : !torch.vtensor<[17,17],f32>, !torch.float -> !torch.vtensor<[17,17],f32>
    %int17_13 = torch.constant.int 17
    %none = torch.constant.none
    %none_14 = torch.constant.none
    %cpu_15 = torch.constant.device "cpu"
    %false_16 = torch.constant.bool false
    %48 = torch.aten.arange %int17_13, %none, %none_14, %cpu_15, %false_16 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[17],si64>
    %int1_17 = torch.constant.int 1
    %int1_18 = torch.constant.int 1
    %49 = torch.aten.add.Scalar %48, %int1_17, %int1_18 : !torch.vtensor<[17],si64>, !torch.int, !torch.int -> !torch.vtensor<[17],si64>
    %int17_19 = torch.constant.int 17
    %int1_20 = torch.constant.int 1
    %50 = torch.prim.ListConstruct %int17_19, %int1_20 : (!torch.int, !torch.int) -> !torch.list<int>
    %51 = torch.aten.view %49, %50 : !torch.vtensor<[17],si64>, !torch.list<int> -> !torch.vtensor<[17,1],si64>
    %52 = torch.aten.lt.Tensor %48, %51 : !torch.vtensor<[17],si64>, !torch.vtensor<[17,1],si64> -> !torch.vtensor<[17,17],i1>
    %float0.000000e00 = torch.constant.float 0.000000e+00
    %int6_21 = torch.constant.int 6
    %int0_22 = torch.constant.int 0
    %cpu_23 = torch.constant.device "cpu"
    %none_24 = torch.constant.none
    %53 = torch.aten.scalar_tensor %float0.000000e00, %int6_21, %int0_22, %cpu_23, %none_24 : !torch.float, !torch.int, !torch.int, !torch.Device, !torch.none -> !torch.vtensor<[],f32>
    %54 = torch.aten.where.self %52, %53, %47 : !torch.vtensor<[17,17],i1>, !torch.vtensor<[],f32>, !torch.vtensor<[17,17],f32> -> !torch.vtensor<[17,17],f32>
    %int2 = torch.constant.int 2
    %55 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %int0_25 = torch.constant.int 0
    %true = torch.constant.bool true
    %result0, %result1 = torch.aten.var_mean.correction %43, %55, %int0_25, %true : !torch.vtensor<[4,17,65],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[4,17,1],f32>, !torch.vtensor<[4,17,1],f32>
    %float1.000000e-04 = torch.constant.float 1.000000e-04
    %int1_26 = torch.constant.int 1
    %56 = torch.aten.add.Scalar %result0, %float1.000000e-04, %int1_26 : !torch.vtensor<[4,17,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,17,1],f32>
    %57 = torch.aten.rsqrt %56 : !torch.vtensor<[4,17,1],f32> -> !torch.vtensor<[4,17,1],f32>
    %int1_27 = torch.constant.int 1
    %58 = torch.aten.sub.Tensor %43, %result1, %int1_27 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[4,17,1],f32>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %59 = torch.aten.mul.Tensor %58, %57 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[4,17,1],f32> -> !torch.vtensor<[4,17,65],f32>
    %60 = torch.aten.mul.Tensor %59, %2 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[65],f32> -> !torch.vtensor<[4,17,65],f32>
    %int1_28 = torch.constant.int 1
    %61 = torch.aten.add.Tensor %60, %3, %int1_28 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[65],f32>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %int-2 = torch.constant.int -2
    %int-1_29 = torch.constant.int -1
    %62 = torch.aten.transpose.int %4, %int-2, %int-1_29 : !torch.vtensor<[65,65],f32>, !torch.int, !torch.int -> !torch.vtensor<[65,65],f32>
    %int68 = torch.constant.int 68
    %int65 = torch.constant.int 65
    %63 = torch.prim.ListConstruct %int68, %int65 : (!torch.int, !torch.int) -> !torch.list<int>
    %64 = torch.aten.view %61, %63 : !torch.vtensor<[4,17,65],f32>, !torch.list<int> -> !torch.vtensor<[68,65],f32>
    %65 = torch.aten.mm %64, %62 : !torch.vtensor<[68,65],f32>, !torch.vtensor<[65,65],f32> -> !torch.vtensor<[68,65],f32>
    %int4 = torch.constant.int 4
    %int17_30 = torch.constant.int 17
    %int65_31 = torch.constant.int 65
    %66 = torch.prim.ListConstruct %int4, %int17_30, %int65_31 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %67 = torch.aten.view %65, %66 : !torch.vtensor<[68,65],f32>, !torch.list<int> -> !torch.vtensor<[4,17,65],f32>
    %int1_32 = torch.constant.int 1
    %68 = torch.aten.add.Tensor %67, %5, %int1_32 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[65],f32>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %float2.773500e-01 = torch.constant.float 0.27735009811261457
    %69 = torch.aten.mul.Scalar %68, %float2.773500e-01 : !torch.vtensor<[4,17,65],f32>, !torch.float -> !torch.vtensor<[4,17,65],f32>
    %int-2_33 = torch.constant.int -2
    %int-1_34 = torch.constant.int -1
    %70 = torch.aten.transpose.int %6, %int-2_33, %int-1_34 : !torch.vtensor<[65,65],f32>, !torch.int, !torch.int -> !torch.vtensor<[65,65],f32>
    %int68_35 = torch.constant.int 68
    %int65_36 = torch.constant.int 65
    %71 = torch.prim.ListConstruct %int68_35, %int65_36 : (!torch.int, !torch.int) -> !torch.list<int>
    %72 = torch.aten.view %61, %71 : !torch.vtensor<[4,17,65],f32>, !torch.list<int> -> !torch.vtensor<[68,65],f32>
    %73 = torch.aten.mm %72, %70 : !torch.vtensor<[68,65],f32>, !torch.vtensor<[65,65],f32> -> !torch.vtensor<[68,65],f32>
    %int4_37 = torch.constant.int 4
    %int17_38 = torch.constant.int 17
    %int65_39 = torch.constant.int 65
    %74 = torch.prim.ListConstruct %int4_37, %int17_38, %int65_39 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %75 = torch.aten.view %73, %74 : !torch.vtensor<[68,65],f32>, !torch.list<int> -> !torch.vtensor<[4,17,65],f32>
    %int1_40 = torch.constant.int 1
    %76 = torch.aten.add.Tensor %75, %7, %int1_40 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[65],f32>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %int4_41 = torch.constant.int 4
    %int-1_42 = torch.constant.int -1
    %int5 = torch.constant.int 5
    %int13 = torch.constant.int 13
    %77 = torch.prim.ListConstruct %int4_41, %int-1_42, %int5, %int13 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %78 = torch.aten.view %76, %77 : !torch.vtensor<[4,17,65],f32>, !torch.list<int> -> !torch.vtensor<[4,17,5,13],f32>
    %int1_43 = torch.constant.int 1
    %int2_44 = torch.constant.int 2
    %79 = torch.aten.transpose.int %78, %int1_43, %int2_44 : !torch.vtensor<[4,17,5,13],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,5,17,13],f32>
    %int0_45 = torch.constant.int 0
    %80 = torch.aten.clone %79, %int0_45 : !torch.vtensor<[4,5,17,13],f32>, !torch.int -> !torch.vtensor<[4,5,17,13],f32>
    %int-2_46 = torch.constant.int -2
    %int-1_47 = torch.constant.int -1
    %81 = torch.aten.transpose.int %8, %int-2_46, %int-1_47 : !torch.vtensor<[65,65],f32>, !torch.int, !torch.int -> !torch.vtensor<[65,65],f32>
    %int68_48 = torch.constant.int 68
    %int65_49 = torch.constant.int 65
    %82 = torch.prim.ListConstruct %int68_48, %int65_49 : (!torch.int, !torch.int) -> !torch.list<int>
    %83 = torch.aten.view %61, %82 : !torch.vtensor<[4,17,65],f32>, !torch.list<int> -> !torch.vtensor<[68,65],f32>
    %84 = torch.aten.mm %83, %81 : !torch.vtensor<[68,65],f32>, !torch.vtensor<[65,65],f32> -> !torch.vtensor<[68,65],f32>
    %int4_50 = torch.constant.int 4
    %int17_51 = torch.constant.int 17
    %int65_52 = torch.constant.int 65
    %85 = torch.prim.ListConstruct %int4_50, %int17_51, %int65_52 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %86 = torch.aten.view %84, %85 : !torch.vtensor<[68,65],f32>, !torch.list<int> -> !torch.vtensor<[4,17,65],f32>
    %int1_53 = torch.constant.int 1
    %87 = torch.aten.add.Tensor %86, %9, %int1_53 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[65],f32>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %int4_54 = torch.constant.int 4
    %int-1_55 = torch.constant.int -1
    %int5_56 = torch.constant.int 5
    %int13_57 = torch.constant.int 13
    %88 = torch.prim.ListConstruct %int4_54, %int-1_55, %int5_56, %int13_57 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %89 = torch.aten.view %87, %88 : !torch.vtensor<[4,17,65],f32>, !torch.list<int> -> !torch.vtensor<[4,17,5,13],f32>
    %int1_58 = torch.constant.int 1
    %int2_59 = torch.constant.int 2
    %90 = torch.aten.transpose.int %89, %int1_58, %int2_59 : !torch.vtensor<[4,17,5,13],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,5,17,13],f32>
    %int0_60 = torch.constant.int 0
    %91 = torch.aten.clone %90, %int0_60 : !torch.vtensor<[4,5,17,13],f32>, !torch.int -> !torch.vtensor<[4,5,17,13],f32>
    %int4_61 = torch.constant.int 4
    %int17_62 = torch.constant.int 17
    %int5_63 = torch.constant.int 5
    %int13_64 = torch.constant.int 13
    %92 = torch.prim.ListConstruct %int4_61, %int17_62, %int5_63, %int13_64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %93 = torch.aten.view %69, %92 : !torch.vtensor<[4,17,65],f32>, !torch.list<int> -> !torch.vtensor<[4,17,5,13],f32>
    %int1_65 = torch.constant.int 1
    %int2_66 = torch.constant.int 2
    %94 = torch.aten.transpose.int %93, %int1_65, %int2_66 : !torch.vtensor<[4,17,5,13],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,5,17,13],f32>
    %int0_67 = torch.constant.int 0
    %95 = torch.aten.clone %94, %int0_67 : !torch.vtensor<[4,5,17,13],f32>, !torch.int -> !torch.vtensor<[4,5,17,13],f32>
    %int20 = torch.constant.int 20
    %int-1_68 = torch.constant.int -1
    %int13_69 = torch.constant.int 13
    %96 = torch.prim.ListConstruct %int20, %int-1_68, %int13_69 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %97 = torch.aten.view %95, %96 : !torch.vtensor<[4,5,17,13],f32>, !torch.list<int> -> !torch.vtensor<[20,17,13],f32>
    %int20_70 = torch.constant.int 20
    %int-1_71 = torch.constant.int -1
    %int13_72 = torch.constant.int 13
    %98 = torch.prim.ListConstruct %int20_70, %int-1_71, %int13_72 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %99 = torch.aten.view %80, %98 : !torch.vtensor<[4,5,17,13],f32>, !torch.list<int> -> !torch.vtensor<[20,17,13],f32>
    %int20_73 = torch.constant.int 20
    %int-1_74 = torch.constant.int -1
    %int13_75 = torch.constant.int 13
    %100 = torch.prim.ListConstruct %int20_73, %int-1_74, %int13_75 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %101 = torch.aten.view %91, %100 : !torch.vtensor<[4,5,17,13],f32>, !torch.list<int> -> !torch.vtensor<[20,17,13],f32>
    %int1_76 = torch.constant.int 1
    %int2_77 = torch.constant.int 2
    %102 = torch.aten.transpose.int %99, %int1_76, %int2_77 : !torch.vtensor<[20,17,13],f32>, !torch.int, !torch.int -> !torch.vtensor<[20,13,17],f32>
    %int20_78 = torch.constant.int 20
    %int17_79 = torch.constant.int 17
    %int13_80 = torch.constant.int 13
    %103 = torch.prim.ListConstruct %int20_78, %int17_79, %int13_80 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_81 = torch.constant.bool false
    %104 = torch.aten.expand %97, %103, %false_81 : !torch.vtensor<[20,17,13],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[20,17,13],f32>
    %int20_82 = torch.constant.int 20
    %int17_83 = torch.constant.int 17
    %int13_84 = torch.constant.int 13
    %105 = torch.prim.ListConstruct %int20_82, %int17_83, %int13_84 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %106 = torch.aten.view %104, %105 : !torch.vtensor<[20,17,13],f32>, !torch.list<int> -> !torch.vtensor<[20,17,13],f32>
    %int20_85 = torch.constant.int 20
    %int13_86 = torch.constant.int 13
    %int17_87 = torch.constant.int 17
    %107 = torch.prim.ListConstruct %int20_85, %int13_86, %int17_87 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_88 = torch.constant.bool false
    %108 = torch.aten.expand %102, %107, %false_88 : !torch.vtensor<[20,13,17],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[20,13,17],f32>
    %int20_89 = torch.constant.int 20
    %int13_90 = torch.constant.int 13
    %int17_91 = torch.constant.int 17
    %109 = torch.prim.ListConstruct %int20_89, %int13_90, %int17_91 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %110 = torch.aten.view %108, %109 : !torch.vtensor<[20,13,17],f32>, !torch.list<int> -> !torch.vtensor<[20,13,17],f32>
    %111 = torch.aten.bmm %106, %110 : !torch.vtensor<[20,17,13],f32>, !torch.vtensor<[20,13,17],f32> -> !torch.vtensor<[20,17,17],f32>
    %int20_92 = torch.constant.int 20
    %int17_93 = torch.constant.int 17
    %int17_94 = torch.constant.int 17
    %112 = torch.prim.ListConstruct %int20_92, %int17_93, %int17_94 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %113 = torch.aten.view %111, %112 : !torch.vtensor<[20,17,17],f32>, !torch.list<int> -> !torch.vtensor<[20,17,17],f32>
    %int4_95 = torch.constant.int 4
    %int5_96 = torch.constant.int 5
    %int17_97 = torch.constant.int 17
    %int17_98 = torch.constant.int 17
    %114 = torch.prim.ListConstruct %int4_95, %int5_96, %int17_97, %int17_98 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %115 = torch.aten.view %113, %114 : !torch.vtensor<[20,17,17],f32>, !torch.list<int> -> !torch.vtensor<[4,5,17,17],f32>
    %int0_99 = torch.constant.int 0
    %116 = torch.aten.unsqueeze %54, %int0_99 : !torch.vtensor<[17,17],f32>, !torch.int -> !torch.vtensor<[1,17,17],f32>
    %int1_100 = torch.constant.int 1
    %117 = torch.aten.unsqueeze %116, %int1_100 : !torch.vtensor<[1,17,17],f32>, !torch.int -> !torch.vtensor<[1,1,17,17],f32>
    %int2_101 = torch.constant.int 2
    %int0_102 = torch.constant.int 0
    %int9223372036854775807_103 = torch.constant.int 9223372036854775807
    %int1_104 = torch.constant.int 1
    %118 = torch.aten.slice.Tensor %117, %int2_101, %int0_102, %int9223372036854775807_103, %int1_104 : !torch.vtensor<[1,1,17,17],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,1,17,17],f32>
    %int3 = torch.constant.int 3
    %int0_105 = torch.constant.int 0
    %int9223372036854775807_106 = torch.constant.int 9223372036854775807
    %int1_107 = torch.constant.int 1
    %119 = torch.aten.slice.Tensor %118, %int3, %int0_105, %int9223372036854775807_106, %int1_107 : !torch.vtensor<[1,1,17,17],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,1,17,17],f32>
    %int4_108 = torch.constant.int 4
    %int1_109 = torch.constant.int 1
    %int17_110 = torch.constant.int 17
    %int17_111 = torch.constant.int 17
    %120 = torch.prim.ListConstruct %int4_108, %int1_109, %int17_110, %int17_111 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_112 = torch.constant.bool false
    %121 = torch.aten.expand %119, %120, %false_112 : !torch.vtensor<[1,1,17,17],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,1,17,17],f32>
    %int1_113 = torch.constant.int 1
    %122 = torch.aten.add.Tensor %115, %121, %int1_113 : !torch.vtensor<[4,5,17,17],f32>, !torch.vtensor<[4,1,17,17],f32>, !torch.int -> !torch.vtensor<[4,5,17,17],f32>
    %int20_114 = torch.constant.int 20
    %int17_115 = torch.constant.int 17
    %int17_116 = torch.constant.int 17
    %123 = torch.prim.ListConstruct %int20_114, %int17_115, %int17_116 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %124 = torch.aten.view %122, %123 : !torch.vtensor<[4,5,17,17],f32>, !torch.list<int> -> !torch.vtensor<[20,17,17],f32>
    %int-1_117 = torch.constant.int -1
    %false_118 = torch.constant.bool false
    %125 = torch.aten._softmax %124, %int-1_117, %false_118 : !torch.vtensor<[20,17,17],f32>, !torch.int, !torch.bool -> !torch.vtensor<[20,17,17],f32>
    %int20_119 = torch.constant.int 20
    %int17_120 = torch.constant.int 17
    %int17_121 = torch.constant.int 17
    %126 = torch.prim.ListConstruct %int20_119, %int17_120, %int17_121 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_122 = torch.constant.bool false
    %127 = torch.aten.expand %125, %126, %false_122 : !torch.vtensor<[20,17,17],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[20,17,17],f32>
    %int20_123 = torch.constant.int 20
    %int17_124 = torch.constant.int 17
    %int17_125 = torch.constant.int 17
    %128 = torch.prim.ListConstruct %int20_123, %int17_124, %int17_125 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %129 = torch.aten.view %127, %128 : !torch.vtensor<[20,17,17],f32>, !torch.list<int> -> !torch.vtensor<[20,17,17],f32>
    %int20_126 = torch.constant.int 20
    %int17_127 = torch.constant.int 17
    %int13_128 = torch.constant.int 13
    %130 = torch.prim.ListConstruct %int20_126, %int17_127, %int13_128 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_129 = torch.constant.bool false
    %131 = torch.aten.expand %101, %130, %false_129 : !torch.vtensor<[20,17,13],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[20,17,13],f32>
    %int20_130 = torch.constant.int 20
    %int17_131 = torch.constant.int 17
    %int13_132 = torch.constant.int 13
    %132 = torch.prim.ListConstruct %int20_130, %int17_131, %int13_132 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %133 = torch.aten.view %131, %132 : !torch.vtensor<[20,17,13],f32>, !torch.list<int> -> !torch.vtensor<[20,17,13],f32>
    %134 = torch.aten.bmm %129, %133 : !torch.vtensor<[20,17,17],f32>, !torch.vtensor<[20,17,13],f32> -> !torch.vtensor<[20,17,13],f32>
    %int20_133 = torch.constant.int 20
    %int17_134 = torch.constant.int 17
    %int13_135 = torch.constant.int 13
    %135 = torch.prim.ListConstruct %int20_133, %int17_134, %int13_135 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %136 = torch.aten.view %134, %135 : !torch.vtensor<[20,17,13],f32>, !torch.list<int> -> !torch.vtensor<[20,17,13],f32>
    %int4_136 = torch.constant.int 4
    %int5_137 = torch.constant.int 5
    %int17_138 = torch.constant.int 17
    %int13_139 = torch.constant.int 13
    %137 = torch.prim.ListConstruct %int4_136, %int5_137, %int17_138, %int13_139 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %138 = torch.aten.view %136, %137 : !torch.vtensor<[20,17,13],f32>, !torch.list<int> -> !torch.vtensor<[4,5,17,13],f32>
    %int1_140 = torch.constant.int 1
    %int2_141 = torch.constant.int 2
    %139 = torch.aten.transpose.int %138, %int1_140, %int2_141 : !torch.vtensor<[4,5,17,13],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,17,5,13],f32>
    %int0_142 = torch.constant.int 0
    %140 = torch.aten.clone %139, %int0_142 : !torch.vtensor<[4,17,5,13],f32>, !torch.int -> !torch.vtensor<[4,17,5,13],f32>
    %int4_143 = torch.constant.int 4
    %int17_144 = torch.constant.int 17
    %int65_145 = torch.constant.int 65
    %141 = torch.prim.ListConstruct %int4_143, %int17_144, %int65_145 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %142 = torch.aten._unsafe_view %140, %141 : !torch.vtensor<[4,17,5,13],f32>, !torch.list<int> -> !torch.vtensor<[4,17,65],f32>
    %int-2_146 = torch.constant.int -2
    %int-1_147 = torch.constant.int -1
    %143 = torch.aten.transpose.int %10, %int-2_146, %int-1_147 : !torch.vtensor<[65,65],f32>, !torch.int, !torch.int -> !torch.vtensor<[65,65],f32>
    %int68_148 = torch.constant.int 68
    %int65_149 = torch.constant.int 65
    %144 = torch.prim.ListConstruct %int68_148, %int65_149 : (!torch.int, !torch.int) -> !torch.list<int>
    %145 = torch.aten.view %142, %144 : !torch.vtensor<[4,17,65],f32>, !torch.list<int> -> !torch.vtensor<[68,65],f32>
    %146 = torch.aten.mm %145, %143 : !torch.vtensor<[68,65],f32>, !torch.vtensor<[65,65],f32> -> !torch.vtensor<[68,65],f32>
    %int4_150 = torch.constant.int 4
    %int17_151 = torch.constant.int 17
    %int65_152 = torch.constant.int 65
    %147 = torch.prim.ListConstruct %int4_150, %int17_151, %int65_152 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %148 = torch.aten.view %146, %147 : !torch.vtensor<[68,65],f32>, !torch.list<int> -> !torch.vtensor<[4,17,65],f32>
    %int1_153 = torch.constant.int 1
    %149 = torch.aten.add.Tensor %148, %11, %int1_153 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[65],f32>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %int1_154 = torch.constant.int 1
    %150 = torch.aten.add.Tensor %43, %149, %int1_154 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[4,17,65],f32>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %int2_155 = torch.constant.int 2
    %151 = torch.prim.ListConstruct %int2_155 : (!torch.int) -> !torch.list<int>
    %int0_156 = torch.constant.int 0
    %true_157 = torch.constant.bool true
    %result0_158, %result1_159 = torch.aten.var_mean.correction %150, %151, %int0_156, %true_157 : !torch.vtensor<[4,17,65],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[4,17,1],f32>, !torch.vtensor<[4,17,1],f32>
    %float1.000000e-04_160 = torch.constant.float 1.000000e-04
    %int1_161 = torch.constant.int 1
    %152 = torch.aten.add.Scalar %result0_158, %float1.000000e-04_160, %int1_161 : !torch.vtensor<[4,17,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,17,1],f32>
    %153 = torch.aten.rsqrt %152 : !torch.vtensor<[4,17,1],f32> -> !torch.vtensor<[4,17,1],f32>
    %int1_162 = torch.constant.int 1
    %154 = torch.aten.sub.Tensor %150, %result1_159, %int1_162 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[4,17,1],f32>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %155 = torch.aten.mul.Tensor %154, %153 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[4,17,1],f32> -> !torch.vtensor<[4,17,65],f32>
    %156 = torch.aten.mul.Tensor %155, %12 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[65],f32> -> !torch.vtensor<[4,17,65],f32>
    %int1_163 = torch.constant.int 1
    %157 = torch.aten.add.Tensor %156, %13, %int1_163 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[65],f32>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %int-2_164 = torch.constant.int -2
    %int-1_165 = torch.constant.int -1
    %158 = torch.aten.transpose.int %14, %int-2_164, %int-1_165 : !torch.vtensor<[7,65],f32>, !torch.int, !torch.int -> !torch.vtensor<[65,7],f32>
    %int68_166 = torch.constant.int 68
    %int65_167 = torch.constant.int 65
    %159 = torch.prim.ListConstruct %int68_166, %int65_167 : (!torch.int, !torch.int) -> !torch.list<int>
    %160 = torch.aten.view %157, %159 : !torch.vtensor<[4,17,65],f32>, !torch.list<int> -> !torch.vtensor<[68,65],f32>
    %161 = torch.aten.mm %160, %158 : !torch.vtensor<[68,65],f32>, !torch.vtensor<[65,7],f32> -> !torch.vtensor<[68,7],f32>
    %int4_168 = torch.constant.int 4
    %int17_169 = torch.constant.int 17
    %int7 = torch.constant.int 7
    %162 = torch.prim.ListConstruct %int4_168, %int17_169, %int7 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %163 = torch.aten.view %161, %162 : !torch.vtensor<[68,7],f32>, !torch.list<int> -> !torch.vtensor<[4,17,7],f32>
    %int1_170 = torch.constant.int 1
    %164 = torch.aten.add.Tensor %163, %15, %int1_170 : !torch.vtensor<[4,17,7],f32>, !torch.vtensor<[7],f32>, !torch.int -> !torch.vtensor<[4,17,7],f32>
    %float1.702000e00 = torch.constant.float 1.702000e+00
    %165 = torch.aten.mul.Scalar %164, %float1.702000e00 : !torch.vtensor<[4,17,7],f32>, !torch.float -> !torch.vtensor<[4,17,7],f32>
    %166 = torch.aten.sigmoid %165 : !torch.vtensor<[4,17,7],f32> -> !torch.vtensor<[4,17,7],f32>
    %167 = torch.aten.mul.Tensor %164, %166 : !torch.vtensor<[4,17,7],f32>, !torch.vtensor<[4,17,7],f32> -> !torch.vtensor<[4,17,7],f32>
    %int-2_171 = torch.constant.int -2
    %int-1_172 = torch.constant.int -1
    %168 = torch.aten.transpose.int %16, %int-2_171, %int-1_172 : !torch.vtensor<[65,7],f32>, !torch.int, !torch.int -> !torch.vtensor<[7,65],f32>
    %int68_173 = torch.constant.int 68
    %int7_174 = torch.constant.int 7
    %169 = torch.prim.ListConstruct %int68_173, %int7_174 : (!torch.int, !torch.int) -> !torch.list<int>
    %170 = torch.aten.view %167, %169 : !torch.vtensor<[4,17,7],f32>, !torch.list<int> -> !torch.vtensor<[68,7],f32>
    %171 = torch.aten.mm %170, %168 : !torch.vtensor<[68,7],f32>, !torch.vtensor<[7,65],f32> -> !torch.vtensor<[68,65],f32>
    %int4_175 = torch.constant.int 4
    %int17_176 = torch.constant.int 17
    %int65_177 = torch.constant.int 65
    %172 = torch.prim.ListConstruct %int4_175, %int17_176, %int65_177 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %173 = torch.aten.view %171, %172 : !torch.vtensor<[68,65],f32>, !torch.list<int> -> !torch.vtensor<[4,17,65],f32>
    %int1_178 = torch.constant.int 1
    %174 = torch.aten.add.Tensor %173, %17, %int1_178 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[65],f32>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %int1_179 = torch.constant.int 1
    %175 = torch.aten.add.Tensor %150, %174, %int1_179 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[4,17,65],f32>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %int2_180 = torch.constant.int 2
    %176 = torch.prim.ListConstruct %int2_180 : (!torch.int) -> !torch.list<int>
    %int0_181 = torch.constant.int 0
    %true_182 = torch.constant.bool true
    %result0_183, %result1_184 = torch.aten.var_mean.correction %175, %176, %int0_181, %true_182 : !torch.vtensor<[4,17,65],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[4,17,1],f32>, !torch.vtensor<[4,17,1],f32>
    %float1.000000e-04_185 = torch.constant.float 1.000000e-04
    %int1_186 = torch.constant.int 1
    %177 = torch.aten.add.Scalar %result0_183, %float1.000000e-04_185, %int1_186 : !torch.vtensor<[4,17,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,17,1],f32>
    %178 = torch.aten.rsqrt %177 : !torch.vtensor<[4,17,1],f32> -> !torch.vtensor<[4,17,1],f32>
    %int1_187 = torch.constant.int 1
    %179 = torch.aten.sub.Tensor %175, %result1_184, %int1_187 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[4,17,1],f32>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %180 = torch.aten.mul.Tensor %179, %178 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[4,17,1],f32> -> !torch.vtensor<[4,17,65],f32>
    %181 = torch.aten.mul.Tensor %180, %18 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[65],f32> -> !torch.vtensor<[4,17,65],f32>
    %int1_188 = torch.constant.int 1
    %182 = torch.aten.add.Tensor %181, %19, %int1_188 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[65],f32>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %int-2_189 = torch.constant.int -2
    %int-1_190 = torch.constant.int -1
    %183 = torch.aten.transpose.int %20, %int-2_189, %int-1_190 : !torch.vtensor<[65,65],f32>, !torch.int, !torch.int -> !torch.vtensor<[65,65],f32>
    %int68_191 = torch.constant.int 68
    %int65_192 = torch.constant.int 65
    %184 = torch.prim.ListConstruct %int68_191, %int65_192 : (!torch.int, !torch.int) -> !torch.list<int>
    %185 = torch.aten.view %182, %184 : !torch.vtensor<[4,17,65],f32>, !torch.list<int> -> !torch.vtensor<[68,65],f32>
    %186 = torch.aten.mm %185, %183 : !torch.vtensor<[68,65],f32>, !torch.vtensor<[65,65],f32> -> !torch.vtensor<[68,65],f32>
    %int4_193 = torch.constant.int 4
    %int17_194 = torch.constant.int 17
    %int65_195 = torch.constant.int 65
    %187 = torch.prim.ListConstruct %int4_193, %int17_194, %int65_195 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %188 = torch.aten.view %186, %187 : !torch.vtensor<[68,65],f32>, !torch.list<int> -> !torch.vtensor<[4,17,65],f32>
    %int1_196 = torch.constant.int 1
    %189 = torch.aten.add.Tensor %188, %21, %int1_196 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[65],f32>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %float2.773500e-01_197 = torch.constant.float 0.27735009811261457
    %190 = torch.aten.mul.Scalar %189, %float2.773500e-01_197 : !torch.vtensor<[4,17,65],f32>, !torch.float -> !torch.vtensor<[4,17,65],f32>
    %int-2_198 = torch.constant.int -2
    %int-1_199 = torch.constant.int -1
    %191 = torch.aten.transpose.int %22, %int-2_198, %int-1_199 : !torch.vtensor<[65,65],f32>, !torch.int, !torch.int -> !torch.vtensor<[65,65],f32>
    %int68_200 = torch.constant.int 68
    %int65_201 = torch.constant.int 65
    %192 = torch.prim.ListConstruct %int68_200, %int65_201 : (!torch.int, !torch.int) -> !torch.list<int>
    %193 = torch.aten.view %182, %192 : !torch.vtensor<[4,17,65],f32>, !torch.list<int> -> !torch.vtensor<[68,65],f32>
    %194 = torch.aten.mm %193, %191 : !torch.vtensor<[68,65],f32>, !torch.vtensor<[65,65],f32> -> !torch.vtensor<[68,65],f32>
    %int4_202 = torch.constant.int 4
    %int17_203 = torch.constant.int 17
    %int65_204 = torch.constant.int 65
    %195 = torch.prim.ListConstruct %int4_202, %int17_203, %int65_204 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %196 = torch.aten.view %194, %195 : !torch.vtensor<[68,65],f32>, !torch.list<int> -> !torch.vtensor<[4,17,65],f32>
    %int1_205 = torch.constant.int 1
    %197 = torch.aten.add.Tensor %196, %23, %int1_205 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[65],f32>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %int4_206 = torch.constant.int 4
    %int-1_207 = torch.constant.int -1
    %int5_208 = torch.constant.int 5
    %int13_209 = torch.constant.int 13
    %198 = torch.prim.ListConstruct %int4_206, %int-1_207, %int5_208, %int13_209 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %199 = torch.aten.view %197, %198 : !torch.vtensor<[4,17,65],f32>, !torch.list<int> -> !torch.vtensor<[4,17,5,13],f32>
    %int1_210 = torch.constant.int 1
    %int2_211 = torch.constant.int 2
    %200 = torch.aten.transpose.int %199, %int1_210, %int2_211 : !torch.vtensor<[4,17,5,13],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,5,17,13],f32>
    %int0_212 = torch.constant.int 0
    %201 = torch.aten.clone %200, %int0_212 : !torch.vtensor<[4,5,17,13],f32>, !torch.int -> !torch.vtensor<[4,5,17,13],f32>
    %int-2_213 = torch.constant.int -2
    %int-1_214 = torch.constant.int -1
    %202 = torch.aten.transpose.int %24, %int-2_213, %int-1_214 : !torch.vtensor<[65,65],f32>, !torch.int, !torch.int -> !torch.vtensor<[65,65],f32>
    %int68_215 = torch.constant.int 68
    %int65_216 = torch.constant.int 65
    %203 = torch.prim.ListConstruct %int68_215, %int65_216 : (!torch.int, !torch.int) -> !torch.list<int>
    %204 = torch.aten.view %182, %203 : !torch.vtensor<[4,17,65],f32>, !torch.list<int> -> !torch.vtensor<[68,65],f32>
    %205 = torch.aten.mm %204, %202 : !torch.vtensor<[68,65],f32>, !torch.vtensor<[65,65],f32> -> !torch.vtensor<[68,65],f32>
    %int4_217 = torch.constant.int 4
    %int17_218 = torch.constant.int 17
    %int65_219 = torch.constant.int 65
    %206 = torch.prim.ListConstruct %int4_217, %int17_218, %int65_219 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %207 = torch.aten.view %205, %206 : !torch.vtensor<[68,65],f32>, !torch.list<int> -> !torch.vtensor<[4,17,65],f32>
    %int1_220 = torch.constant.int 1
    %208 = torch.aten.add.Tensor %207, %25, %int1_220 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[65],f32>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %int4_221 = torch.constant.int 4
    %int-1_222 = torch.constant.int -1
    %int5_223 = torch.constant.int 5
    %int13_224 = torch.constant.int 13
    %209 = torch.prim.ListConstruct %int4_221, %int-1_222, %int5_223, %int13_224 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %210 = torch.aten.view %208, %209 : !torch.vtensor<[4,17,65],f32>, !torch.list<int> -> !torch.vtensor<[4,17,5,13],f32>
    %int1_225 = torch.constant.int 1
    %int2_226 = torch.constant.int 2
    %211 = torch.aten.transpose.int %210, %int1_225, %int2_226 : !torch.vtensor<[4,17,5,13],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,5,17,13],f32>
    %int0_227 = torch.constant.int 0
    %212 = torch.aten.clone %211, %int0_227 : !torch.vtensor<[4,5,17,13],f32>, !torch.int -> !torch.vtensor<[4,5,17,13],f32>
    %int4_228 = torch.constant.int 4
    %int17_229 = torch.constant.int 17
    %int5_230 = torch.constant.int 5
    %int13_231 = torch.constant.int 13
    %213 = torch.prim.ListConstruct %int4_228, %int17_229, %int5_230, %int13_231 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %214 = torch.aten.view %190, %213 : !torch.vtensor<[4,17,65],f32>, !torch.list<int> -> !torch.vtensor<[4,17,5,13],f32>
    %int1_232 = torch.constant.int 1
    %int2_233 = torch.constant.int 2
    %215 = torch.aten.transpose.int %214, %int1_232, %int2_233 : !torch.vtensor<[4,17,5,13],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,5,17,13],f32>
    %int0_234 = torch.constant.int 0
    %216 = torch.aten.clone %215, %int0_234 : !torch.vtensor<[4,5,17,13],f32>, !torch.int -> !torch.vtensor<[4,5,17,13],f32>
    %int20_235 = torch.constant.int 20
    %int-1_236 = torch.constant.int -1
    %int13_237 = torch.constant.int 13
    %217 = torch.prim.ListConstruct %int20_235, %int-1_236, %int13_237 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %218 = torch.aten.view %216, %217 : !torch.vtensor<[4,5,17,13],f32>, !torch.list<int> -> !torch.vtensor<[20,17,13],f32>
    %int20_238 = torch.constant.int 20
    %int-1_239 = torch.constant.int -1
    %int13_240 = torch.constant.int 13
    %219 = torch.prim.ListConstruct %int20_238, %int-1_239, %int13_240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %220 = torch.aten.view %201, %219 : !torch.vtensor<[4,5,17,13],f32>, !torch.list<int> -> !torch.vtensor<[20,17,13],f32>
    %int20_241 = torch.constant.int 20
    %int-1_242 = torch.constant.int -1
    %int13_243 = torch.constant.int 13
    %221 = torch.prim.ListConstruct %int20_241, %int-1_242, %int13_243 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %222 = torch.aten.view %212, %221 : !torch.vtensor<[4,5,17,13],f32>, !torch.list<int> -> !torch.vtensor<[20,17,13],f32>
    %int1_244 = torch.constant.int 1
    %int2_245 = torch.constant.int 2
    %223 = torch.aten.transpose.int %220, %int1_244, %int2_245 : !torch.vtensor<[20,17,13],f32>, !torch.int, !torch.int -> !torch.vtensor<[20,13,17],f32>
    %int20_246 = torch.constant.int 20
    %int17_247 = torch.constant.int 17
    %int13_248 = torch.constant.int 13
    %224 = torch.prim.ListConstruct %int20_246, %int17_247, %int13_248 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_249 = torch.constant.bool false
    %225 = torch.aten.expand %218, %224, %false_249 : !torch.vtensor<[20,17,13],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[20,17,13],f32>
    %int20_250 = torch.constant.int 20
    %int17_251 = torch.constant.int 17
    %int13_252 = torch.constant.int 13
    %226 = torch.prim.ListConstruct %int20_250, %int17_251, %int13_252 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %227 = torch.aten.view %225, %226 : !torch.vtensor<[20,17,13],f32>, !torch.list<int> -> !torch.vtensor<[20,17,13],f32>
    %int20_253 = torch.constant.int 20
    %int13_254 = torch.constant.int 13
    %int17_255 = torch.constant.int 17
    %228 = torch.prim.ListConstruct %int20_253, %int13_254, %int17_255 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_256 = torch.constant.bool false
    %229 = torch.aten.expand %223, %228, %false_256 : !torch.vtensor<[20,13,17],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[20,13,17],f32>
    %int20_257 = torch.constant.int 20
    %int13_258 = torch.constant.int 13
    %int17_259 = torch.constant.int 17
    %230 = torch.prim.ListConstruct %int20_257, %int13_258, %int17_259 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %231 = torch.aten.view %229, %230 : !torch.vtensor<[20,13,17],f32>, !torch.list<int> -> !torch.vtensor<[20,13,17],f32>
    %232 = torch.aten.bmm %227, %231 : !torch.vtensor<[20,17,13],f32>, !torch.vtensor<[20,13,17],f32> -> !torch.vtensor<[20,17,17],f32>
    %int20_260 = torch.constant.int 20
    %int17_261 = torch.constant.int 17
    %int17_262 = torch.constant.int 17
    %233 = torch.prim.ListConstruct %int20_260, %int17_261, %int17_262 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %234 = torch.aten.view %232, %233 : !torch.vtensor<[20,17,17],f32>, !torch.list<int> -> !torch.vtensor<[20,17,17],f32>
    %int4_263 = torch.constant.int 4
    %int5_264 = torch.constant.int 5
    %int17_265 = torch.constant.int 17
    %int17_266 = torch.constant.int 17
    %235 = torch.prim.ListConstruct %int4_263, %int5_264, %int17_265, %int17_266 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %236 = torch.aten.view %234, %235 : !torch.vtensor<[20,17,17],f32>, !torch.list<int> -> !torch.vtensor<[4,5,17,17],f32>
    %int1_267 = torch.constant.int 1
    %237 = torch.aten.add.Tensor %236, %121, %int1_267 : !torch.vtensor<[4,5,17,17],f32>, !torch.vtensor<[4,1,17,17],f32>, !torch.int -> !torch.vtensor<[4,5,17,17],f32>
    %int20_268 = torch.constant.int 20
    %int17_269 = torch.constant.int 17
    %int17_270 = torch.constant.int 17
    %238 = torch.prim.ListConstruct %int20_268, %int17_269, %int17_270 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %239 = torch.aten.view %237, %238 : !torch.vtensor<[4,5,17,17],f32>, !torch.list<int> -> !torch.vtensor<[20,17,17],f32>
    %int-1_271 = torch.constant.int -1
    %false_272 = torch.constant.bool false
    %240 = torch.aten._softmax %239, %int-1_271, %false_272 : !torch.vtensor<[20,17,17],f32>, !torch.int, !torch.bool -> !torch.vtensor<[20,17,17],f32>
    %int20_273 = torch.constant.int 20
    %int17_274 = torch.constant.int 17
    %int17_275 = torch.constant.int 17
    %241 = torch.prim.ListConstruct %int20_273, %int17_274, %int17_275 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_276 = torch.constant.bool false
    %242 = torch.aten.expand %240, %241, %false_276 : !torch.vtensor<[20,17,17],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[20,17,17],f32>
    %int20_277 = torch.constant.int 20
    %int17_278 = torch.constant.int 17
    %int17_279 = torch.constant.int 17
    %243 = torch.prim.ListConstruct %int20_277, %int17_278, %int17_279 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %244 = torch.aten.view %242, %243 : !torch.vtensor<[20,17,17],f32>, !torch.list<int> -> !torch.vtensor<[20,17,17],f32>
    %int20_280 = torch.constant.int 20
    %int17_281 = torch.constant.int 17
    %int13_282 = torch.constant.int 13
    %245 = torch.prim.ListConstruct %int20_280, %int17_281, %int13_282 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_283 = torch.constant.bool false
    %246 = torch.aten.expand %222, %245, %false_283 : !torch.vtensor<[20,17,13],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[20,17,13],f32>
    %int20_284 = torch.constant.int 20
    %int17_285 = torch.constant.int 17
    %int13_286 = torch.constant.int 13
    %247 = torch.prim.ListConstruct %int20_284, %int17_285, %int13_286 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %248 = torch.aten.view %246, %247 : !torch.vtensor<[20,17,13],f32>, !torch.list<int> -> !torch.vtensor<[20,17,13],f32>
    %249 = torch.aten.bmm %244, %248 : !torch.vtensor<[20,17,17],f32>, !torch.vtensor<[20,17,13],f32> -> !torch.vtensor<[20,17,13],f32>
    %int20_287 = torch.constant.int 20
    %int17_288 = torch.constant.int 17
    %int13_289 = torch.constant.int 13
    %250 = torch.prim.ListConstruct %int20_287, %int17_288, %int13_289 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %251 = torch.aten.view %249, %250 : !torch.vtensor<[20,17,13],f32>, !torch.list<int> -> !torch.vtensor<[20,17,13],f32>
    %int4_290 = torch.constant.int 4
    %int5_291 = torch.constant.int 5
    %int17_292 = torch.constant.int 17
    %int13_293 = torch.constant.int 13
    %252 = torch.prim.ListConstruct %int4_290, %int5_291, %int17_292, %int13_293 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %253 = torch.aten.view %251, %252 : !torch.vtensor<[20,17,13],f32>, !torch.list<int> -> !torch.vtensor<[4,5,17,13],f32>
    %int1_294 = torch.constant.int 1
    %int2_295 = torch.constant.int 2
    %254 = torch.aten.transpose.int %253, %int1_294, %int2_295 : !torch.vtensor<[4,5,17,13],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,17,5,13],f32>
    %int0_296 = torch.constant.int 0
    %255 = torch.aten.clone %254, %int0_296 : !torch.vtensor<[4,17,5,13],f32>, !torch.int -> !torch.vtensor<[4,17,5,13],f32>
    %int4_297 = torch.constant.int 4
    %int17_298 = torch.constant.int 17
    %int65_299 = torch.constant.int 65
    %256 = torch.prim.ListConstruct %int4_297, %int17_298, %int65_299 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %257 = torch.aten._unsafe_view %255, %256 : !torch.vtensor<[4,17,5,13],f32>, !torch.list<int> -> !torch.vtensor<[4,17,65],f32>
    %int-2_300 = torch.constant.int -2
    %int-1_301 = torch.constant.int -1
    %258 = torch.aten.transpose.int %26, %int-2_300, %int-1_301 : !torch.vtensor<[65,65],f32>, !torch.int, !torch.int -> !torch.vtensor<[65,65],f32>
    %int68_302 = torch.constant.int 68
    %int65_303 = torch.constant.int 65
    %259 = torch.prim.ListConstruct %int68_302, %int65_303 : (!torch.int, !torch.int) -> !torch.list<int>
    %260 = torch.aten.view %257, %259 : !torch.vtensor<[4,17,65],f32>, !torch.list<int> -> !torch.vtensor<[68,65],f32>
    %261 = torch.aten.mm %260, %258 : !torch.vtensor<[68,65],f32>, !torch.vtensor<[65,65],f32> -> !torch.vtensor<[68,65],f32>
    %int4_304 = torch.constant.int 4
    %int17_305 = torch.constant.int 17
    %int65_306 = torch.constant.int 65
    %262 = torch.prim.ListConstruct %int4_304, %int17_305, %int65_306 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %263 = torch.aten.view %261, %262 : !torch.vtensor<[68,65],f32>, !torch.list<int> -> !torch.vtensor<[4,17,65],f32>
    %int1_307 = torch.constant.int 1
    %264 = torch.aten.add.Tensor %263, %27, %int1_307 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[65],f32>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %int1_308 = torch.constant.int 1
    %265 = torch.aten.add.Tensor %175, %264, %int1_308 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[4,17,65],f32>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %int2_309 = torch.constant.int 2
    %266 = torch.prim.ListConstruct %int2_309 : (!torch.int) -> !torch.list<int>
    %int0_310 = torch.constant.int 0
    %true_311 = torch.constant.bool true
    %result0_312, %result1_313 = torch.aten.var_mean.correction %265, %266, %int0_310, %true_311 : !torch.vtensor<[4,17,65],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[4,17,1],f32>, !torch.vtensor<[4,17,1],f32>
    %float1.000000e-04_314 = torch.constant.float 1.000000e-04
    %int1_315 = torch.constant.int 1
    %267 = torch.aten.add.Scalar %result0_312, %float1.000000e-04_314, %int1_315 : !torch.vtensor<[4,17,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,17,1],f32>
    %268 = torch.aten.rsqrt %267 : !torch.vtensor<[4,17,1],f32> -> !torch.vtensor<[4,17,1],f32>
    %int1_316 = torch.constant.int 1
    %269 = torch.aten.sub.Tensor %265, %result1_313, %int1_316 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[4,17,1],f32>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %270 = torch.aten.mul.Tensor %269, %268 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[4,17,1],f32> -> !torch.vtensor<[4,17,65],f32>
    %271 = torch.aten.mul.Tensor %270, %28 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[65],f32> -> !torch.vtensor<[4,17,65],f32>
    %int1_317 = torch.constant.int 1
    %272 = torch.aten.add.Tensor %271, %29, %int1_317 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[65],f32>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %int-2_318 = torch.constant.int -2
    %int-1_319 = torch.constant.int -1
    %273 = torch.aten.transpose.int %30, %int-2_318, %int-1_319 : !torch.vtensor<[7,65],f32>, !torch.int, !torch.int -> !torch.vtensor<[65,7],f32>
    %int68_320 = torch.constant.int 68
    %int65_321 = torch.constant.int 65
    %274 = torch.prim.ListConstruct %int68_320, %int65_321 : (!torch.int, !torch.int) -> !torch.list<int>
    %275 = torch.aten.view %272, %274 : !torch.vtensor<[4,17,65],f32>, !torch.list<int> -> !torch.vtensor<[68,65],f32>
    %276 = torch.aten.mm %275, %273 : !torch.vtensor<[68,65],f32>, !torch.vtensor<[65,7],f32> -> !torch.vtensor<[68,7],f32>
    %int4_322 = torch.constant.int 4
    %int17_323 = torch.constant.int 17
    %int7_324 = torch.constant.int 7
    %277 = torch.prim.ListConstruct %int4_322, %int17_323, %int7_324 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %278 = torch.aten.view %276, %277 : !torch.vtensor<[68,7],f32>, !torch.list<int> -> !torch.vtensor<[4,17,7],f32>
    %int1_325 = torch.constant.int 1
    %279 = torch.aten.add.Tensor %278, %31, %int1_325 : !torch.vtensor<[4,17,7],f32>, !torch.vtensor<[7],f32>, !torch.int -> !torch.vtensor<[4,17,7],f32>
    %float1.702000e00_326 = torch.constant.float 1.702000e+00
    %280 = torch.aten.mul.Scalar %279, %float1.702000e00_326 : !torch.vtensor<[4,17,7],f32>, !torch.float -> !torch.vtensor<[4,17,7],f32>
    %281 = torch.aten.sigmoid %280 : !torch.vtensor<[4,17,7],f32> -> !torch.vtensor<[4,17,7],f32>
    %282 = torch.aten.mul.Tensor %279, %281 : !torch.vtensor<[4,17,7],f32>, !torch.vtensor<[4,17,7],f32> -> !torch.vtensor<[4,17,7],f32>
    %int-2_327 = torch.constant.int -2
    %int-1_328 = torch.constant.int -1
    %283 = torch.aten.transpose.int %32, %int-2_327, %int-1_328 : !torch.vtensor<[65,7],f32>, !torch.int, !torch.int -> !torch.vtensor<[7,65],f32>
    %int68_329 = torch.constant.int 68
    %int7_330 = torch.constant.int 7
    %284 = torch.prim.ListConstruct %int68_329, %int7_330 : (!torch.int, !torch.int) -> !torch.list<int>
    %285 = torch.aten.view %282, %284 : !torch.vtensor<[4,17,7],f32>, !torch.list<int> -> !torch.vtensor<[68,7],f32>
    %286 = torch.aten.mm %285, %283 : !torch.vtensor<[68,7],f32>, !torch.vtensor<[7,65],f32> -> !torch.vtensor<[68,65],f32>
    %int4_331 = torch.constant.int 4
    %int17_332 = torch.constant.int 17
    %int65_333 = torch.constant.int 65
    %287 = torch.prim.ListConstruct %int4_331, %int17_332, %int65_333 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %288 = torch.aten.view %286, %287 : !torch.vtensor<[68,65],f32>, !torch.list<int> -> !torch.vtensor<[4,17,65],f32>
    %int1_334 = torch.constant.int 1
    %289 = torch.aten.add.Tensor %288, %33, %int1_334 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[65],f32>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %int1_335 = torch.constant.int 1
    %290 = torch.aten.add.Tensor %265, %289, %int1_335 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[4,17,65],f32>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %int2_336 = torch.constant.int 2
    %291 = torch.prim.ListConstruct %int2_336 : (!torch.int) -> !torch.list<int>
    %int0_337 = torch.constant.int 0
    %true_338 = torch.constant.bool true
    %result0_339, %result1_340 = torch.aten.var_mean.correction %290, %291, %int0_337, %true_338 : !torch.vtensor<[4,17,65],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[4,17,1],f32>, !torch.vtensor<[4,17,1],f32>
    %float1.000000e-04_341 = torch.constant.float 1.000000e-04
    %int1_342 = torch.constant.int 1
    %292 = torch.aten.add.Scalar %result0_339, %float1.000000e-04_341, %int1_342 : !torch.vtensor<[4,17,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,17,1],f32>
    %293 = torch.aten.rsqrt %292 : !torch.vtensor<[4,17,1],f32> -> !torch.vtensor<[4,17,1],f32>
    %int1_343 = torch.constant.int 1
    %294 = torch.aten.sub.Tensor %290, %result1_340, %int1_343 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[4,17,1],f32>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %295 = torch.aten.mul.Tensor %294, %293 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[4,17,1],f32> -> !torch.vtensor<[4,17,65],f32>
    %296 = torch.aten.mul.Tensor %295, %34 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[65],f32> -> !torch.vtensor<[4,17,65],f32>
    %int1_344 = torch.constant.int 1
    %297 = torch.aten.add.Tensor %296, %35, %int1_344 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[65],f32>, !torch.int -> !torch.vtensor<[4,17,65],f32>
    %int4_345 = torch.constant.int 4
    %none_346 = torch.constant.none
    %none_347 = torch.constant.none
    %cpu_348 = torch.constant.device "cpu"
    %false_349 = torch.constant.bool false
    %298 = torch.aten.arange %int4_345, %none_346, %none_347, %cpu_348, %false_349 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[4],si64>
    %int10 = torch.constant.int 10
    %299 = torch.aten.eq.Scalar %38, %int10 : !torch.vtensor<[4,17],si64>, !torch.int -> !torch.vtensor<[4,17],i1>
    %int3_350 = torch.constant.int 3
    %300 = torch.prims.convert_element_type %299, %int3_350 : !torch.vtensor<[4,17],i1>, !torch.int -> !torch.vtensor<[4,17],si32>
    %int-1_351 = torch.constant.int -1
    %false_352 = torch.constant.bool false
    %301 = torch.aten.argmax %300, %int-1_351, %false_352 : !torch.vtensor<[4,17],si32>, !torch.int, !torch.bool -> !torch.vtensor<[4],si64>
    %302 = torch.prim.ListConstruct %298, %301 : (!torch.vtensor<[4],si64>, !torch.vtensor<[4],si64>) -> !torch.list<optional<vtensor>>
    %303 = torch.aten.index.Tensor %297, %302 : !torch.vtensor<[4,17,65],f32>, !torch.list<optional<vtensor>> -> !torch.vtensor<[4,65],f32>
    return %297, %303 : !torch.vtensor<[4,17,65],f32>, !torch.vtensor<[4,65],f32>
  }
}

{-#
  dialect_resources: {
    builtin: {
      torch_tensor_1_17_torch.int64: "0x0800000000000000000000000100000000000000020000000000000003000000000000000400000000000000050000000000000006000000000000000700000000000000080000000000000009000000000000000A000000000000000B000000000000000C000000000000000D000000000000000E000000000000000F000000000000001000000000000000"
    }
  }
#-}
